{
  "description": "The **AWSSupport-AnalyzeEBSResourceUsage** automation runbook is used to analyze resource usage on Amazon Elastic Block Store (Amazon EBS). It analyzes volume usage and identifies abandoned volumes, images, and snapshots in a given AWS Region.\n\n## How does it work?\n\nThe runbook performs the following four tasks:\n\n1. Verifies that an Amazon Simple Storage Service (Amazon S3) bucket exists, or creates a new Amazon S3 bucket.\n\n2. Gathers all the Amazon EBS volumes in the available state.\n\n3. Gathers all Amazon EBS snapshots for which source volume has been deleted.\n\n4. Gathers all Amazon Machine Images (AMIs) which are not in use by any non-terminated Amazon Elastic Compute Cloud (Amazon EC2) instances.\n\nThe runbook generates CSV reports and stores them in a user-provided Amazon S3 bucket. The provided bucket should be secured following AWS security best practices as outlined in the end. If the user provided Amazon S3 bucket does not exist in the account, the runbook creates a new Amazon S3 bucket with the name format **<User-provided-name>-awssupport-YYYY-MM-DD**, encrypted with a custom AWS Key Management Service (AWS KMS) key, with object versioning enabled, blocked public access, and require requests to use SSL/TLS.\n\nIf you want to specify your own Amazon S3 bucket, please make sure it is configured following these best practices:\n\n- Block public access to the bucket (set `IsPublic` to `False`).\n- Turn on Amazon S3 access logging.\n- Allow only SSL requests to your bucket.\n- Turn on object versioning.\n- Use an AWS Key Management Service (AWS KMS) key to encrypt your bucket.\n\n## Important:\nUsing this runbook might incur extra charges against your account for the creation of S3 bucket and objects. See [Amazon S3 Pricing](https://aws.amazon.com/s3/pricing/) for more details on the charges that may be incurred.",
  "schemaVersion": "0.3",
  "assumeRole": "{{ AutomationAssumeRole }}",
  "outputs": [
    "reportError.failureMessage",
    "verifyOrCreateS3bucket.createdNewBucket",
    "gatherAmiDetails.gatherAmiDetailsOutput",
    "gatherVolumeDetails.gatherVolumeDetailsOutput",
    "gatherSnapshotDetails.gatherSnapshotDetailsOutput"
  ],
  "parameters": {
    "S3BucketName": {
      "type": "AWS::S3::Bucket::Name",
      "description": "(Optional) The Amazon S3 bucket in your account to upload the report to. Ensure the bucket policy does not grant unnecessary read/write permissions to parties that do not need access to the collected logs. If the bucket specified does not exist in the account, then automation creates a new bucket in the Region where automation is initiated with the name format <User-provided-name>-awssupport-YYYY-MM-DD, encrypted with a custom AWS KMS key.",
      "default": ""
    },
    "CustomerManagedKmsKeyArn": {
      "type": "String",
      "description": "(Optional) The custom AWS KMS key Amazon Resource Name (ARN) for encrypting the new Amazon S3 bucket that will create if the bucket specified does not exist in the account. Automation fails if the bucket creation is attempted without specifying a custom AWS KMS key ARN.",
      "allowedPattern": "^$|^arn:(aws|aws-cn|aws-us-gov|aws-iso(-[a-z])?):kms:[-a-z0-9]+:\\d{12}:key/[-a-z0-9]*$",
      "default": ""
    },
    "AutomationAssumeRole": {
      "type": "AWS::IAM::Role::Arn",
      "description": "(Optional) The Amazon Resource Name (ARN) of the AWS Identity and Access Management (IAM) role that allows Systems Manager Automation to perform the actions on your behalf. If no role is specified, Systems Manager Automation uses the permissions of the user that starts this runbook.",
      "default": ""
    }
  },
  "mainSteps": [
    {
      "name": "checkConcurrency",
      "action": "aws:executeScript",
      "description": "Ensures there is only one execution of this runbook in the region. If the runbook finds another execution in progress, it returns an error and ends.",
      "timeoutSeconds": 600,
      "isCritical": true,
      "onFailure": "step:reportError",
      "inputs": {
        "Handler": "check_concurrency.script_handler",
        "Runtime": "python3.11",
        "Attachment": "artifact.zip"
      },
      "outputs": [
        {
          "Name": "NoExecutionFound",
          "Selector": "$.Payload.NoExecutionFound",
          "Type": "String"
        }
      ],
      "nextStep": "verifyOrCreateS3bucket"
    },
    {
      "name": "reportError",
      "description": "Retrieves the error message from `checkConcurrency` step and exposes it in the execution output.",
      "action": "aws:executeScript",
      "isCritical": true,
      "onFailure": "Abort",
      "timeoutSeconds": 180,
      "inputs": {
        "Handler": "report_error.script_handler",
        "Runtime": "python3.11",
        "Attachment": "artifact.zip"
      },
      "outputs": [
        {
          "Name": "failureMessage",
          "Selector": "$.Payload.FailureMessage",
          "Type": "String"
        }
      ],
      "isEnd": true
    },
    {
      "name": "verifyOrCreateS3bucket",
      "description": "Verifies if the Amazon S3 bucket exists. If not, it creates a new Amazon S3 bucket in the Region where automation is initiated with the name format <User-provided-name>-awssupport-YYYY-MM-DD, encrypted with a custom AWS KMS key.",
      "action": "aws:executeScript",
      "isCritical": true,
      "onFailure": "Abort",
      "inputs": {
        "InputPayload": {
          "existingBucket": "{{ S3BucketName }}",
          "accountId": "{{ global:ACCOUNT_ID }}",
          "date": "{{ global:DATE }}",
          "region": "{{ global:REGION }}",
          "kmsKeyArn": "{{ CustomerManagedKmsKeyArn }}"
        },
        "Handler": "verify_or_create_s3_bucket.script_handler",
        "Runtime": "python3.11",
        "Attachment": "artifact.zip"
      },
      "outputs": [
        {
          "Name": "bucketName",
          "Selector": "$.Payload.bucket_name",
          "Type": "String"
        },
        {
          "Name": "createdNewBucket",
          "Selector": "$.Payload.created_new_bucket",
          "Type": "Boolean"
        }
      ],
      "nextStep": "gatherAmiDetails"
    },
    {
      "name": "gatherAmiDetails",
      "action": "aws:executeScript",
      "isCritical": true,
      "onFailure": "Abort",
      "nextStep": "gatherVolumeDetails",
      "inputs": {
        "InputPayload": {
          "s3Bucket": "{{ verifyOrCreateS3bucket.bucketName }}",
          "accountId": "{{ global:ACCOUNT_ID }}",
          "region": "{{ global:REGION }}"
        },
        "Handler": "gather_ami_details.script_handler",
        "Runtime": "python3.11",
        "Attachment": "artifact.zip"
      },
      "description": "Searches for AMIs, which are not in use by any Amazon EC2 instances, generates the report with the name format <region>-images.csv, and uploads it to the Amazon S3 bucket.",
      "outputs": [
        {
          "Name": "gatherAmiDetailsOutput",
          "Selector": "$.Payload.gatherAmiDetailsOutput",
          "Type": "String"
        }
      ]
    },
    {
      "name": "gatherVolumeDetails",
      "action": "aws:executeScript",
      "isCritical": true,
      "onFailure": "Abort",
      "nextStep": "gatherSnapshotDetails",
      "inputs": {
        "InputPayload": {
          "s3Bucket": "{{ verifyOrCreateS3bucket.bucketName }}",
          "accountId": "{{ global:ACCOUNT_ID }}",
          "region": "{{ global:REGION }}"
        },
        "Handler": "gather_volume_details.script_handler",
        "Runtime": "python3.11",
        "Attachment": "artifact.zip"
      },
      "description": "Verifies Amazon EBS volumes in the available state, generates the report with the name format <region>-volume.csv, and uploads it in an Amazon S3 bucket.",
      "outputs": [
        {
          "Name": "gatherVolumeDetailsOutput",
          "Selector": "$.Payload.gatherVolumeDetailsOutput",
          "Type": "String"
        }
      ]
    },
    {
      "name": "gatherSnapshotDetails",
      "description": "Looks for the Amazon EBS snapshots of the Amazon EBS volumes that are deleted already, generates the report with the name format <region>-snapshot.csv, and uploads it to Amazon S3 bucket.",
      "action": "aws:executeScript",
      "isCritical": true,
      "isEnd": true,
      "onFailure": "Abort",
      "inputs": {
        "InputPayload": {
          "s3Bucket": "{{ verifyOrCreateS3bucket.bucketName }}",
          "accountId": "{{ global:ACCOUNT_ID }}",
          "region": "{{ global:REGION }}"
        },
        "Handler": "gather_snapshot_details.script_handler",
        "Runtime": "python3.11",
        "Attachment": "artifact.zip"
      },
      "outputs": [
        {
          "Name": "gatherSnapshotDetailsOutput",
          "Selector": "$.Payload.gatherSnapshotDetailsOutput",
          "Type": "String"
        }
      ]
    }
  ],
  "files": {
    "artifact.zip": {
      "checksums": {
        "SHA256": "db6f544229ee071e8f8d85f4c71dd354082b62b3b289abe7dc4171f7834f3bac"
      }
    }
  }
}
