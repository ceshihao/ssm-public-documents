{
  "schemaVersion": "0.3",
  "description": "The AWSSupport-CollectSAPHANALogs runbook collects system logs from SAP HANA on Amazon Elastic Compute Cloud (Amazon EC2) instance that is part of SAP on AWS deployment. The Amazon EC2 instance must be [managed by AWS Systems Manager](https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-setting-up-ec2.html).\n\n## Runbook parameters\n* **InstanceID**: The ID of the Amazon EC2 instance running the SAP workload.\n* **Acknowledgement**: Select 'Yes' to acknowledge that the runbook may install required packages listed below\n* **S3LogDestination**: Optional Amazon S3 bucket name to upload logs to Amazon S3. If not provided, logs are stored locally on the instance. The Amazon S3 bucket must not be public and must belong to the same AWS account.\n* **InstallAWSCLI**: Select 'Yes' to automatically install AWS CLI if it's missing on the instance.\n\n**Installs the following packages if missing on the instance**\n## SUSE Linux Enterprise Server: \n* supportutils\n* yast2-support\n* supportutils-plugin-suse-public-cloud \n* supportutils-plugin-ha-sap \n* crmsh\n* unzip\n* curl\n* aws-cli (optional)\n\n### Red Hat Enterprise Linux : \n* sos\n* crm_report\n* unzip\n* curl\n* aws-cli (optional)\n\n### Supported Operating Systems\n* Red Hat Enterprise Linux 8.4+\n* SUSE Linux Enterprise Server 12.5\n* SUSE Linux Enterprise Server 15.3+\n\n## Please note the following:\n* For HA clusters, the runbook also collects additional HA logs from the last 7 days using *crm report* command.\n* Storing logs in Amazon S3 incurs standard S3 storage and request charges.\n\n### *Important*: requires at least 200MB available disk space on the '/var/log' partition.",
  "assumeRole": "{{ AutomationAssumeRole }}",
  "parameters": {
    "AutomationAssumeRole": {
      "type": "AWS::IAM::Role::Arn",
      "description": "(Optional) The Amazon Resource Name (ARN) of the AWS Identity and Access Management (IAM) role that allows Systems Manager Automation to perform the actions on your behalf. If no role is specified, Systems Manager Automation uses the permissions of the user that starts this runbook.",
      "default": "",
      "allowedPattern": "^$|^arn:(aws|aws-cn|aws-us-gov|aws-iso|aws-iso-b):iam::[0-9]{12}:role/[a-zA-Z0-9+=,.@_-]+$"
    },
    "InstanceID": {
      "type": "AWS::EC2::Instance::Id",
      "description": "(Required) The ID of the Amazon EC2 instance running SAP workload from which HA logs should be collected."
    },
    "S3LogDestination": {
      "type": "AWS::S3::Bucket::Name",
      "description": "(Optional) The name of the Amazon S3 bucket to which logs are uploaded. If not provided, logs are stored in instance local storage.",
      "default": ""
    },
    "S3Prefix": {
      "type": "String",
      "description": "(Optional) The Amazon S3 bucket prefix where logs are stored. If not provided, defaults to 'AWSSupport-CollectSAPHANALogs'.",
      "allowedPattern": "^$|^[a-zA-Z0-9][-./a-zA-Z0-9]{0,255}$",
      "default": "AWSSupport-CollectSAPHANALogs"
    },
    "Acknowledgement": {
      "type": "String",
      "description": "(Required) I acknowledge that this runbook may install additional packages in the target Amazon EC2 instance for log collection.",
      "allowedValues": [
        "Yes"
      ]
    },
    "InstallAWSCLI": {
      "type": "String",
      "description": "(Optional) Whether to install AWS CLI on the instance. If 'Yes', the runbook installs AWS CLI if it is not installed on the instance.",
      "allowedValues": [
        "Yes",
        "No"
      ],
      "default": "No"
    }
  },
  "mainSteps": [
    {
      "name": "AssertInstanceIsSSMManaged",
      "description": "Asserts if the target Amazon EC2 instance is managed by AWS Systems Manager.",
      "action": "aws:assertAwsResourceProperty",
      "onFailure": "Abort",
      "onCancel": "Abort",
      "timeoutSeconds": 60,
      "maxAttempts": 3,
      "inputs": {
        "Service": "ssm",
        "Api": "DescribeInstanceInformation",
        "InstanceInformationFilterList": [
          {
            "key": "InstanceIds",
            "valueSet": [
              "{{ InstanceID }}"
            ]
          }
        ],
        "PropertySelector": "$.InstanceInformationList[0].PingStatus",
        "DesiredValues": [
          "Online"
        ]
      },
      "nextStep": "GetInstanceInformation"
    },
    {
      "name": "GetInstanceInformation",
      "description": "Retrieves information about the specified Amazon EC2 instance.",
      "action": "aws:executeAwsApi",
      "onFailure": "Abort",
      "onCancel": "Abort",
      "timeoutSeconds": 60,
      "maxAttempts": 3,
      "inputs": {
        "Service": "ssm",
        "Api": "DescribeInstanceInformation",
        "InstanceInformationFilterList": [
          {
            "key": "InstanceIds",
            "valueSet": [
              "{{ InstanceID }}"
            ]
          }
        ]
      },
      "outputs": [
        {
          "Name": "PlatformName",
          "Selector": "$.InstanceInformationList[0].PlatformName",
          "Type": "String"
        }
      ],
      "nextStep": "CollectLogs"
    },
    {
      "name": "CollectLogs",
      "description": "Collects logs from SLES/RHEL instances. The script checks if required packages (sos, supportutils, crm_report, crmsh) are installed and installs them if needed. Standard log collection uses supportconfig for SLES and sos report for RHEL. For HA clusters, it also collects additional HA logs from the last 7 days using crm report command.",
      "action": "aws:runCommand",
      "onFailure": "Abort",
      "inputs": {
        "DocumentName": "AWS-RunShellScript",
        "InstanceIds": [
          "{{ InstanceID }}"
        ],
        "Parameters": {
          "commands": [
            "#!/bin/bash",
            "",
            "# Copyright 2025 Amazon.com, Inc. or its affiliates. All Rights Reserved.",
            "# SPDX-License-Identifier: LicenseRef-.amazon.com.-AmznSL-1.0",
            "# Licensed under the Amazon Software License  http://aws.amazon.com/asl/",
            "",
            "export LANG=\"C\"",
            "",
            "set -euo pipefail",
            "",
            "# Script metadata",
            "SCRIPT_NAME=$(basename \"$0\") ",
            "",
            "readonly SCRIPT_NAME",
            "readonly SCRIPT_VERSION=\"1.0.0\"",
            "readonly LOG_DIR=\"/var/log/ec2_linux_ha_logs\"",
            "LOG_ARCHIVE=\"$LOG_DIR/ec2_linux_ha_logs_archive_$(date +'%Y-%m-%d_%H-%M-%S').tar.gz\"",
            "readonly LOG_ARCHIVE",
            "readonly LOG_OUTPUT_FILE=\"$LOG_DIR/generated_log_archives_path.txt\"",
            "readonly USAGE=\"Usage: $SCRIPT_NAME [-h|--help] [-v|--version] - Collect HA logs for Linux systems\"",
            "LOG_FILE=\"/var/log/collect_ec2_linux_ha_logs_$(date +'%Y-%m-%d_%H-%M-%S').log\"",
            "readonly LOG_FILE",
            "",
            "# Memory threshold: 18TB in kilobytes (corrected calculation)",
            "DEFAULT_MEMORY_THRESHOLD=$((18 * 1024 * 1024 * 1024))",
            "readonly DEFAULT_MEMORY_THRESHOLD",
            "",
            "# OS-specific package arrays",
            "readonly RHEL_PACKAGES=(\"sos\")",
            "readonly SLES_PACKAGES=(\"supportutils\" \"yast2-support\" \"supportutils-plugin-suse-public-cloud\")",
            "readonly SLES_HA_PACKAGES=(\"crmsh\" \"supportutils-plugin-ha-sap\")",
            "",
            "# Array to store all collected log paths",
            "declare -a log_paths=()",
            "",
            "# OS detection variables",
            "OS_TYPE=\"\"",
            "PKG_MANAGER=\"\"",
            "",
            "# Logging functions",
            "log() {",
            "    local -r level=\"$1\"",
            "    local -r message=\"$2\"",
            "    local -r timestamp=$(date +\"%Y-%m-%d %H:%M:%S\")",
            "    # Write INFO messages to stdout, ERROR/WARN to stderr",
            "    if [[ \"$level\" == \"INFO\" ]]; then",
            "        echo \"${timestamp} [${level}] [$SCRIPT_NAME] ${message}\"",
            "    else",
            "        echo >&2 \"${timestamp} [${level}] [$SCRIPT_NAME] ${message}\"",
            "    fi",
            "    echo \"${timestamp} [${level}] [$SCRIPT_NAME] ${message}\" >> \"$LOG_FILE\"",
            "}",
            "",
            "log_info()    { log \"INFO\" \"$@\"; }",
            "log_warn()    { log \"WARN\" \"$@\"; }",
            "log_error()   { log \"ERROR\" \"$@\"; }",
            "",
            "# Error handling and cleanup",
            "cleanup() {",
            "    local -r exit_code=\"$?\"",
            "    local cleanup_failed=false",
            "    local failed_paths=()",
            "    ",
            "    if [[ $exit_code -ne 0 ]]; then",
            "        log_error \"Script failed with exit code $exit_code\"",
            "        # Clean up any temporary files on failure",
            "        if [[ ${#log_paths[@]} -gt 0 ]]; then",
            "            for path in \"${log_paths[@]}\"; do",
            "                if [[ -f \"$path\" && \"$path\" != \"$LOG_ARCHIVE\" ]]; then",
            "                    if ! rm -f \"$path\" 2>/dev/null; then",
            "                        cleanup_failed=true",
            "                        failed_paths+=(\"$path\")",
            "                        log_error \"Failed to remove log file: $path\"",
            "                    else",
            "                        log \"Successfully removed log file: $path\"",
            "                    fi",
            "                fi",
            "            done",
            "        fi",
            "        ",
            "        # Inform user about manual cleanup if automatic cleanup failed",
            "        if [[ \"$cleanup_failed\" == true ]]; then",
            "            log_error \"Automatic cleanup of some log files failed.\"",
            "            log_error \"Please manually remove the following log files from the instance:\"",
            "            log_error \"\"",
            "            for failed_path in \"${failed_paths[@]}\"; do",
            "                log_error \"  $failed_path\"",
            "            done",
            "            log_error \"\"",
            "            log_error \"You can remove them by running the following command(s):\"",
            "            if [[ ${#failed_paths[@]} -eq 1 ]]; then",
            "                log_error \"  rm -f \\\"${failed_paths[0]}\\\"\"",
            "            else",
            "                log_error \"  rm -f \\\\\"",
            "                for i in \"${!failed_paths[@]}\"; do",
            "                    if [[ $i -eq $((${#failed_paths[@]} - 1)) ]]; then",
            "                        log_error \"    \\\"${failed_paths[$i]}\\\"\"",
            "                    else",
            "                        log_error \"    \\\"${failed_paths[$i]}\\\" \\\\\"",
            "                    fi",
            "                done",
            "            fi",
            "            log_error \"\"",
            "            log_error \"Or remove all failed log files at once:\"",
            "            printf -v failed_paths_string ' \"%s\"' \"${failed_paths[@]}\"",
            "            log_error \"  rm -f${failed_paths_string}\"",
            "        else",
            "            log \"All temporary log files cleaned up successfully\"",
            "        fi",
            "    fi",
            "}",
            "",
            "die() {",
            "    log_error \"$1\"",
            "    exit \"${2:-1}\"",
            "}",
            "",
            "trap cleanup EXIT",
            "",
            "# OS Detection",
            "detect_os() {",
            "    log_info \"Detecting operating system...\"",
            "    ",
            "    if [[ -f /etc/os-release ]]; then",
            "        source /etc/os-release",
            "        case \"$ID\" in",
            "            rhel)",
            "                OS_TYPE=\"RHEL\"",
            "                if command -v dnf >/dev/null; then",
            "                    PKG_MANAGER=\"dnf\"",
            "                elif command -v yum >/dev/null; then",
            "                    PKG_MANAGER=\"yum\"",
            "                else",
            "                    die \"Neither dnf nor yum package manager found\"",
            "                fi",
            "                ;;",
            "            sles)",
            "                OS_TYPE=\"SLES\"",
            "                PKG_MANAGER=\"zypper\"",
            "                ;;",
            "            *)",
            "                die \"Unsupported OS: $ID. Only SUSE Linux Enterprise Server (SLES) and Red Hat Enterprise Linux (RHEL) distributions are supported\"",
            "                ;;",
            "        esac",
            "    else",
            "        die \"Cannot detect OS - /etc/os-release not found\"",
            "    fi",
            "    ",
            "    log_info \"Detected OS: $OS_TYPE, Package Manager: $PKG_MANAGER\"",
            "}",
            "",
            "# Check for running log collection processes",
            "check_running_processes() {",
            "    log_info \"Checking for running log collection processes...\"",
            "    ",
            "    local running_processes=()",
            "    ",
            "    # Check for sos/sosreport processes",
            "    if pgrep -f \"sos\" >/dev/null 2>&1; then",
            "        running_processes+=(\"sos/sosreport\")",
            "    fi",
            "    ",
            "    # Check for supportconfig processes",
            "    if pgrep -f \"supportconfig\" >/dev/null 2>&1; then",
            "        running_processes+=(\"supportconfig\")",
            "    fi",
            "    ",
            "    # Check for crm_report processes",
            "    if pgrep -f \"crm_report\" >/dev/null 2>&1; then",
            "        running_processes+=(\"crm_report\")",
            "    fi",
            "    ",
            "    if [[ ${#running_processes[@]} -gt 0 ]]; then",
            "        log_error \"Found running log collection processes: ${running_processes[*]}\"",
            "        log_error \"Another log collection is already in progress. Exiting to avoid conflicts.\"",
            "        die \"Log collection already running\"",
            "    fi",
            "}",
            "",
            "# Prerequisite checks",
            "check_prerequisites() {",
            "    log_info \"Checking prerequisites...\"",
            "    detect_os",
            "    command -v rpm >/dev/null || die \"rpm is required but not installed\"",
            "}",
            "",
            "# Check if the script is running as root",
            "is_root() {",
            "    log_info \"Checking root privileges...\"",
            "    if [[ \"${EUID}\" -ne 0 ]]; then",
            "        die \"This script must be run as root\"",
            "    fi",
            "}",
            "",
            "check_disk_space() {",
            "    [[ $(($(df /var/log | awk 'NR==2 {print $4}') / 1024)) -ge 200 ]] || die \"Insufficient disk space. At least 200MiB of available disk space is required\"",
            "}",
            "",
            "# Cluster and system checks",
            "is_ha_cluster() {    ",
            "    if [[ -f \"/etc/corosync/corosync.conf\" ]]; then",
            "        log_info \"System is running as HA cluster (corosync.conf found). Collecting High Availability cluster logs...\"",
            "        return 0",
            "    else",
            "        log_info \"System is not running as HA cluster (corosync.conf not found)\"",
            "        return 1",
            "    fi",
            "}",
            "",
            "check_instance_memory() {",
            "    log_info \"Checking instance memory...\"",
            "    local total_mem_kb",
            "    total_mem_kb=$(grep MemTotal /proc/meminfo | awk '{print $2}') || die \"Failed to get memory info\"",
            "",
            "    if [ \"$total_mem_kb\" -lt \"$DEFAULT_MEMORY_THRESHOLD\" ]; then",
            "        log_info \"Memory is less than 18TB, Running standard supportconfig collection\"",
            "        return 0",
            "    else",
            "        log_warn \"Memory is greater than or equal to 18TB, Running modified supportconfig collection for high-memory instances\"",
            "        return 1",
            "    fi",
            "}",
            "",
            "get_rhel_version() {",
            "    log_info \"Detecting RHEL version...\"",
            "    if [ -f /etc/redhat-release ]; then",
            "        local version",
            "        version=$(rpm -q --qf \"%{VERSION}\" \"$(rpm -q --whatprovides redhat-release)\")",
            "        log_info \"Detected RHEL version: $version\"",
            "        echo \"$version\"",
            "    else",
            "        die \"Could not determine RHEL version\"",
            "    fi",
            "}",
            "",
            "# Package management",
            "install_package() {",
            "    local -r package=\"$1\"",
            "    log_info \"Installing package: $package\"",
            "    ",
            "    case \"$OS_TYPE\" in",
            "        RHEL)",
            "            if ! $PKG_MANAGER -y --quiet install \"$package\" >/dev/null; then",
            "                die \"Failed to install package: $package\"",
            "            fi",
            "            ;;",
            "        SLES)",
            "            if ! zypper --non-interactive --quiet in \"$package\" >/dev/null; then",
            "                die \"Failed to install package: $package\"",
            "            fi",
            "            ;;",
            "    esac",
            "}",
            "",
            "check_utilities() {",
            "    log_info \"Checking required utilities...\"",
            "",
            "    case \"$OS_TYPE\" in",
            "        RHEL)",
            "            for package in \"${RHEL_PACKAGES[@]}\"; do",
            "                if ! rpm -q \"$package\" &>/dev/null; then",
            "                    log_info \"Package $package not found. Installing...\"",
            "                    install_package \"$package\"",
            "                fi",
            "            done",
            "            ;;",
            "        SLES)",
            "            for package in \"${SLES_PACKAGES[@]}\"; do",
            "                if ! rpm -q \"$package\" &>/dev/null; then",
            "                    log_info \"Package $package not found. Installing...\"",
            "                    install_package \"$package\"",
            "                fi",
            "            done",
            "",
            "            if is_ha_cluster; then",
            "                log_info \"Checking required HA utilities...\"",
            "                for package in \"${SLES_HA_PACKAGES[@]}\"; do",
            "                    if ! rpm -q \"$package\" &>/dev/null; then",
            "                        log_info \"Package $package not found. Installing...\"",
            "                        install_package \"$package\"",
            "                    fi",
            "                done",
            "            fi",
            "            ;;",
            "    esac",
            "}",
            "",
            "# Log collection",
            "create_log_output_file() {    ",
            "    # Check if file exists and remove it",
            "    if [ -f \"$LOG_OUTPUT_FILE\" ]; then",
            "        rm \"$LOG_OUTPUT_FILE\"",
            "    fi",
            "    ",
            "    # Create empty file",
            "    touch \"$LOG_OUTPUT_FILE\"",
            "}",
            "",
            "# Collect HA logs",
            "collect_ha_logs() {",
            "    local base_path",
            "    base_path=\"$LOG_DIR/crm_report-$(date +%Y%m%d)\"",
            "    local crm_report_output",
            "    local exit_code",
            "    ",
            "    case \"$OS_TYPE\" in",
            "        RHEL)",
            "            base_path=\"${base_path}-$(date +%s)\"",
            "            ;;",
            "        SLES)",
            "            # SLES uses different timestamp format",
            "            ;;",
            "    esac",
            "    ",
            "    # Collect HA logs for the past 7 days",
            "    crm_report_output=$(crm_report -u root -p \"passw.*\" -f \"$(date --date='7 days ago' +%Y-%m-%d' '%H:%M:%S)\" -S \"${base_path}\" 2>&1)",
            "    exit_code=$?",
            "    ",
            "    if [ $exit_code -ne 0 ]; then",
            "        log_warn \"Failed to collect HA cluster logs. Exit code: $exit_code\"",
            "        log_warn \"$crm_report_output\"",
            "        return 1",
            "    fi",
            "    ",
            "    # Extract the log_path from the output",
            "    local crm_log_path",
            "    case \"$OS_TYPE\" in",
            "        RHEL)",
            "            crm_log_path=$(echo \"$crm_report_output\" | grep -o \"$LOG_DIR/.*\\.tar\\.bz2\" || echo \"$base_path\")",
            "            ;;",
            "        SLES)",
            "            crm_log_path=$(echo \"$crm_report_output\" | grep \"saved in\" | awk -F \"saved in \" '{print $2}')",
            "            ;;",
            "    esac",
            "    ",
            "    if [[ -z \"$crm_log_path\" ]]; then",
            "        log_warn \"Could not find the generated HA logs archive path in command output\"",
            "        log_warn \"$crm_report_output\"",
            "        return 1",
            "    fi",
            "    ",
            "    if [[ ! -f \"$crm_log_path\" ]]; then",
            "        log_warn \"HA logs archive not found at expected path: $crm_log_path\"",
            "        log_warn \"Full command output:\"",
            "        log_warn \"$crm_report_output\"",
            "        return 1",
            "    fi",
            "    ",
            "    log_info \"HA cluster logs collected successfully at: $crm_log_path\"",
            "    log_paths+=(\"$crm_log_path\")",
            "    return 0",
            "}",
            "",
            "# Collect system logs",
            "collect_system_logs() {",
            "    log_info \"Starting system log collection...\"",
            "",
            "    case \"$OS_TYPE\" in",
            "        RHEL)",
            "            collect_rhel_system_logs",
            "            ;;",
            "        SLES)",
            "            collect_sles_system_logs",
            "            ;;",
            "    esac",
            "}",
            "",
            "collect_rhel_system_logs() {",
            "    local sos_output",
            "    local log_path",
            "    local rhel_release",
            "    local version_string",
            "    local rhel_major_version",
            "    local rhel_minor_version",
            "",
            "    rhel_release=$(cat /etc/redhat-release)",
            "",
            "    # Extract the version number (e.g., \"7.9\")",
            "    version_string=$(echo \"$rhel_release\" | grep -oP \"release \\K[0-9]+\\.[0-9]+\")",
            "",
            "    # Extract major and minor versions",
            "    rhel_major_version=$(echo \"$version_string\" | cut -d'.' -f1)",
            "    rhel_minor_version=$(echo \"$version_string\" | cut -d'.' -f2)",
            "",
            "    if [[ $rhel_major_version -lt 8 ]] || [[ $rhel_major_version -eq 8 && $rhel_minor_version -lt 4 ]]; then",
            "        log_info \"Running sosreport for RHEL $version_string\"",
            "        sos_output=$(sosreport --threads=1 --no-report --batch --log-size=50 --all-logs --tmp-dir $LOG_DIR 2>&1) || die \"sosreport failed\"",
            "    else",
            "        log_info \"Running sos report for RHEL $version_string\"",
            "        sos_output=$(sos report --threads=1 --no-report --batch --log-size=50 --all-logs --tmp-dir $LOG_DIR 2>&1) || die \"sos report failed\"",
            "    fi",
            "",
            "    # Extract the path to the generated archive",
            "    log_path=$(echo \"$sos_output\" | grep -o \"$LOG_DIR/sosreport-.*\\.tar\\.[gx]z\" || echo \"\")",
            "",
            "    if [[ -z \"$log_path\" || ! -f \"$log_path\" ]]; then",
            "        log_warn \"Could not find the generated sosreport archive\"",
            "        log_warn \"$sos_output\"",
            "        die \"SOS report log not found at expected path\"",
            "    fi",
            "",
            "    log_info \"System logs collected successfully at: $log_path\"",
            "    log_paths+=(\"$log_path\")",
            "}",
            "",
            "collect_sles_system_logs() {",
            "    # Create default config file if it doesn't exist",
            "    if [[ ! -f /etc/supportconfig.conf ]]; then",
            "        log_info \"Creating default supportconfig configuration\"",
            "        supportconfig -C > /dev/null || die \"Failed to create default supportconfig configuration\"",
            "    fi",
            "",
            "    local supportconfig_output",
            "    local log_path",
            "    local temp_archive_name",
            "    temp_archive_name=\"supportconfig_$(date +'%Y-%m-%d_%H-%M-%S')\"",
            "",
            "    if check_instance_memory; then",
            "        supportconfig_output=$(supportconfig -l -R \"$LOG_DIR\" -B \"$temp_archive_name\" 2>&1) || die \"Supportconfig failed\"",
            "    else",
            "        supportconfig_output=$(supportconfig -l -R \"$LOG_DIR\" -x OFILES,PROC -B \"$temp_archive_name\" 2>&1) || die \"Supportconfig failed\"",
            "    fi",
            "",
            "    log_path=$(echo \"$supportconfig_output\" | grep \"Log file tar ball:\" | awk '{print $NF}')",
            "",
            "    if [[ ! -f \"$log_path\" ]]; then",
            "        log_warn \"Supportconfig log not found at expected path: $log_path\"",
            "        log_warn \"Full supportconfig output:\"",
            "        log_warn \"$supportconfig_output\"",
            "        die \"Supportconfig log not found at expected path\"",
            "    fi",
            "",
            "    log_info \"System logs collected successfully at: $log_path\"",
            "    log_paths+=(\"$log_path\")",
            "}",
            "",
            "# Create final compressed archive",
            "create_final_archive() {    ",
            "    if [[ ${#log_paths[@]} -eq 0 ]]; then",
            "        die \"No log files collected to archive\"",
            "    fi",
            "",
            "    # Create archive with all log paths",
            "    if tar -czf \"$LOG_ARCHIVE\" \"${log_paths[@]}\" 2>/dev/null; then",
            "        log_info \"Successfully created log archive: $LOG_ARCHIVE\"",
            "    else",
            "        log_warn \"Some log files may not be accessible, creating archive with available files\"",
            "        if tar -czf \"$LOG_ARCHIVE\" --ignore-failed-read \"${log_paths[@]}\" 2>/dev/null; then",
            "            log_info \"Created log archive with available files: $LOG_ARCHIVE\"",
            "        else",
            "            die \"Failed to create log archive\"",
            "        fi",
            "    fi",
            "    ",
            "    # Verify archive was created",
            "    if [[ -f \"$LOG_ARCHIVE\" ]]; then       ",
            "        # Write the final archive path to output file",
            "        echo \"$LOG_ARCHIVE\" > \"$LOG_OUTPUT_FILE\"        ",
            "        # Clean up individual log files (except our own log file and the final archive)",
            "        log_info \"Cleaning up temporary files\"",
            "        for path in \"${log_paths[@]}\"; do",
            "            if [[ -f \"$path\" && \"$path\" != \"$LOG_FILE\" && \"$path\" != \"$LOG_ARCHIVE\" ]]; then",
            "                rm -f \"$path\" 2>/dev/null || log_warn \"Could not remove temporary file: $path\"",
            "            fi",
            "        done",
            "        ",
            "        return 0",
            "    else",
            "        die \"Archive creation failed - file not found at $LOG_ARCHIVE\"",
            "    fi",
            "}",
            "",
            "# Main log collection orchestrator",
            "collect_all_logs() {",
            "    log_info \"Starting comprehensive log collection...\"",
            "    ",
            "    # Create Output directory",
            "    mkdir -p $LOG_DIR",
            "    # Create log output file",
            "    create_log_output_file",
            "    ",
            "    # Collect system logs",
            "    if ! collect_system_logs; then",
            "        die \"Failed to collect system logs\"",
            "    fi",
            "",
            "    # If HA, collect HA logs",
            "    if is_ha_cluster; then",
            "        if ! collect_ha_logs; then",
            "            log_warn \"Failed to collect HA logs, continuing with regular log collection\"",
            "        fi",
            "    fi",
            "    ",
            "    # Create final compressed archive",
            "    create_final_archive    ",
            "    return 0",
            "}",
            "",
            "parse_options() {",
            "    while [[ $# -gt 0 ]]; do",
            "        case $1 in",
            "            -h|--help)",
            "                echo \"$USAGE\"",
            "                echo \"Collects system logs for EC2 Linux HA clusters on RHEL and SLES systems\"",
            "                echo",
            "                echo \"Options:\"",
            "                echo \"  -h, --help     Show this help message\"",
            "                echo \"  -v, --version  Show script version\"",
            "                echo",
            "                echo \"Output:\"",
            "                echo \"  Creates a compressed archive containing all collected logs in $LOG_OUTPUT_FILE\"",
            "                echo",
            "                echo \"Memory Handling (SLES only):\"",
            "                echo \"  - Standard collection for instances < 18TB memory\"",
            "                echo \"  - Modified collection (excludes OFILES,PROC) for instances >= 18TB memory\"",
            "                exit 0",
            "                ;;",
            "            -v|--version)",
            "                echo \"$SCRIPT_VERSION\"",
            "                exit 0",
            "                ;;",
            "            *)",
            "                log_error \"Unknown option: $1\"",
            "                echo \"$USAGE\" >&2",
            "                exit 1",
            "                ;;",
            "        esac",
            "    done",
            "}",
            "",
            "# Main function",
            "main() {",
            "    # Parse command line arguments",
            "    parse_options \"$@\"",
            "",
            "    log_info \"Starting script version $SCRIPT_VERSION\"",
            "",
            "    check_prerequisites",
            "    is_root",
            "    check_running_processes",
            "    check_disk_space",
            "    check_utilities",
            "",
            "    if ! collect_all_logs; then",
            "        die \"Failed to collect and archive logs\"",
            "    fi",
            "",
            "    log_info \"Log collection completed successfully. Final archive: $LOG_ARCHIVE\"",
            "}",
            "",
            "# Execute main function",
            "if [[ \"${BASH_SOURCE[0]}\" == \"${0}\" ]]; then",
            "    main \"$@\"",
            "fi"
          ]
        }
      },
      "outputs": [
        {
          "Name": "LogOutput",
          "Selector": "$.Output",
          "Type": "String"
        }
      ],
      "nextStep": "BranchOnS3BucketProvided",
      "onCancel": "Abort",
      "timeoutSeconds": 3600
    },
    {
      "name": "BranchOnS3BucketProvided",
      "description": "Branches the execution based on whether an Amazon S3 bucket is provided.",
      "action": "aws:branch",
      "onFailure": "step:GenerateReport",
      "onCancel": "Abort",
      "timeoutSeconds": 60,
      "maxAttempts": 1,
      "inputs": {
        "Choices": [
          {
            "Variable": "{{ S3LogDestination }}",
            "StringEquals": "",
            "NextStep": "GenerateReport"
          }
        ],
        "Default": "CheckS3BucketPublicStatus"
      },
      "nextStep": "CheckS3BucketPublicStatus"
    },
    {
      "name": "CheckS3BucketPublicStatus",
      "description": "Checks if the Amazon S3 bucket specified in S3LogDestination is configured with server-side encryption (SSE), and if it allows anonymous, or public read or write access permissions. Also verifies that the actual bucket owner is the same as the expected bucket owner.",
      "action": "aws:executeScript",
      "onFailure": "step:GenerateReport",
      "onCancel": "Abort",
      "isCritical": false,
      "maxAttempts": 2,
      "timeoutSeconds": 60,
      "inputs": {
        "InputPayload": {
          "Bucket": "{{ S3LogDestination }}",
          "AutomationAssumeRole": "{{ AutomationAssumeRole }}"
        },
        "Handler": "s3_bucket_public_status.check_bucket_public_status",
        "Runtime": "python3.11",
        "Attachment": "artifact_1.zip"
      },
      "outputs": [
        {
          "Name": "bucketLocation",
          "Selector": "$.Payload.location",
          "Type": "String"
        },
        {
          "Name": "bucketOwner",
          "Selector": "$.Payload.bucket_owner",
          "Type": "String"
        }
      ],
      "nextStep": "UploadLogsToS3"
    },
    {
      "name": "UploadLogsToS3",
      "description": "Uploads the collected logs to the specified Amazon S3 bucket. If InstallAWSCLI is set to Yes and AWS CLI is not installed, the script installs it for both x86 and arm64 architectures. The script also configures AWS CLI to enable log upload.",
      "action": "aws:runCommand",
      "onFailure": "Continue",
      "onCancel": "Abort",
      "inputs": {
        "DocumentName": "AWS-RunShellScript",
        "InstanceIds": [
          "{{ InstanceID }}"
        ],
        "Parameters": {
          "commands": [
            "#!/bin/bash",
            "# Copyright 2025 Amazon.com, Inc. or its affiliates. All Rights Reserved.",
            "# SPDX-License-Identifier: LicenseRef-.amazon.com.-AmznSL-1.0",
            "# Licensed under the Amazon Software License  http://aws.amazon.com/asl/",
            "",
            "# Purpose:",
            "# This script is designed to be used as part of an AWS Systems Manager Automation runbook.",
            "# It uploads collected logs to S3 bucket with support for SLES and RHEL instances.",
            "",
            "set -euo pipefail",
            "",
            "# Script metadata",
            "readonly LOG_DIR=\"/var/log/ec2_linux_ha_logs\"",
            "readonly LOG_PATHS_FILE=\"$LOG_DIR/generated_log_archives_path.txt\"",
            "",
            "# Function to log messages",
            "log() {",
            "    echo \"$(date '+%Y-%m-%d %H:%M:%S') - $1\" | tee -a /var/log/aws-log-uploader.log",
            "}",
            "",
            "# Function to handle errors",
            "handle_error() {",
            "    local exit_code=$?",
            "    local line_number=$1",
            "    log \"Error on line $line_number: Command exited with status $exit_code\"",
            "    echo \"{\\\"Output\\\": \\\"Failed to upload logs to S3. Error on line $line_number with exit code $exit_code\\\"}\"",
            "    exit $exit_code",
            "}",
            "",
            "# Set up error handling",
            "trap 'handle_error $LINENO' ERR",
            "",
            "# Function to detect OS distribution",
            "detect_os() {",
            "    # Use /etc/os-release as primary method (works on all modern systems)",
            "    if [ -f /etc/os-release ]; then",
            "        . /etc/os-release",
            "        case \"$ID\" in",
            "            \"sles\"|\"opensuse\"|\"opensuse-leap\"|\"opensuse-tumbleweed\")",
            "                echo \"sles\"",
            "                ;;",
            "            \"rhel\"|\"centos\"|\"rocky\"|\"almalinux\"|\"fedora\")",
            "                echo \"rhel\"",
            "                ;;",
            "            \"ubuntu\"|\"debian\")",
            "                echo \"debian\"",
            "                ;;",
            "            *)",
            "                # For any other ID, return it as-is",
            "                echo \"$ID\"",
            "                ;;",
            "        esac",
            "    # Fallback for very old systems",
            "    elif [ -f /etc/redhat-release ]; then",
            "        echo \"rhel\"",
            "    elif [ -f /etc/SuSE-release ]; then",
            "        # This will only match very old SUSE systems (SLES 11 and older)",
            "        echo \"sles\"",
            "    elif [ -f /etc/SUSE-brand ]; then",
            "        # Alternative SUSE detection for systems without os-release",
            "        echo \"sles\"",
            "    else",
            "        echo \"unknown\"",
            "    fi",
            "}",
            "",
            "# Function to check if AWS CLI is installed",
            "check_aws_cli() {",
            "    if command -v aws >/dev/null 2>&1; then",
            "        log \"AWS CLI is already installed: $(aws --version)\"",
            "        return 0",
            "    else",
            "        log \"AWS CLI is not installed\"",
            "        return 1",
            "    fi",
            "}",
            "",
            "# Function to install packages based on OS",
            "install_package() {",
            "    local package=\"$1\"",
            "    local os_type=\"$2\"",
            "",
            "    log \"Installing $package on $os_type system...\"",
            "",
            "    case \"$os_type\" in",
            "        \"rhel\")",
            "            if command -v dnf >/dev/null 2>&1; then",
            "                dnf install -y \"$package\" >/dev/null 2>&1",
            "            elif command -v yum >/dev/null 2>&1; then",
            "                yum install -y \"$package\" >/dev/null 2>&1",
            "            else",
            "                log \"No package manager found for RHEL-based system\"",
            "                return 1",
            "            fi",
            "            ;;",
            "        \"sles\"|\"opensuse\"|\"opensuse-leap\"|\"opensuse-tumbleweed\")",
            "            zypper install -y \"$package\" >/dev/null 2>&1",
            "            ;;",
            "        *)",
            "            log \"Unsupported OS: $os_type\"",
            "            return 1",
            "            ;;",
            "    esac",
            "}",
            "",
            "# Function to install AWS CLI",
            "install_aws_cli() {",
            "    log \"Installing AWS CLI...\"",
            "",
            "    # Detect OS",
            "    OS_TYPE=$(detect_os)",
            "    log \"Detected OS: $OS_TYPE\"",
            "",
            "    # Determine architecture",
            "    ARCH=$(uname -m)",
            "    log \"Detected architecture: $ARCH\"",
            "",
            "    # Install required packages",
            "    if ! command -v curl >/dev/null 2>&1; then",
            "        log \"Installing curl...\"",
            "        install_package \"curl\" \"$OS_TYPE\"",
            "    fi",
            "",
            "    if ! command -v unzip >/dev/null 2>&1; then",
            "        log \"Installing unzip...\"",
            "        install_package \"unzip\" \"$OS_TYPE\"",
            "    fi",
            "",
            "",
            "    # Create temporary directory for installation",
            "    TEMP_DIR=$(mktemp -d)",
            "    if [ -n \"$TEMP_DIR\" ] && [ -d \"$TEMP_DIR\" ]; then",
            "        cd \"$TEMP_DIR\"",
            "    else",
            "        echo \"Error: Temporary directory creation failed\"",
            "        exit 1",
            "    fi",
            "",
            "    # Download AWS CLI based on architecture",
            "    if [ \"$ARCH\" == \"x86_64\" ]; then",
            "        log \"Downloading AWS CLI for x86_64 architecture\"",
            "        curl -s \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"",
            "    elif [ \"$ARCH\" == \"aarch64\" ] || [ \"$ARCH\" == \"arm64\" ]; then",
            "        log \"Downloading AWS CLI for arm64 architecture\"",
            "        curl -s \"https://awscli.amazonaws.com/awscli-exe-linux-aarch64.zip\" -o \"awscliv2.zip\"",
            "    else",
            "        log \"Unsupported architecture: $ARCH\"",
            "        echo \"{\\\"Output\\\": \\\"Failed to install AWS CLI. Unsupported architecture: $ARCH\\\"}\"",
            "        exit 1",
            "    fi",
            "",
            "    # Extract and install",
            "    unzip -q awscliv2.zip",
            "    ./aws/install",
            "",
            "    # Clean up",
            "    cd - >/dev/null",
            "    if ! rm -rf \"$TEMP_DIR\"; then",
            "        log \"Warning: Failed to remove temporary directory: $TEMP_DIR. You can manually remove it by running: rm -rf $TEMP_DIR\"",
            "    fi",
            "",
            "    # Verify installation",
            "    if ! command -v aws >/dev/null 2>&1; then",
            "        log \"AWS CLI installation failed\"",
            "        echo \"{\\\"Output\\\": \\\"Failed to install AWS CLI\\\"}\"",
            "        exit 1",
            "    fi",
            "",
            "    log \"AWS CLI installed successfully\"",
            "    aws --version",
            "}",
            "",
            "# Function to configure AWS CLI",
            "configure_aws_cli() {",
            "    log \"Configuring AWS CLI...\"",
            "",
            "    # Get instance region from instance metadata service",
            "    TOKEN=$(curl -s -X PUT \"http://169.254.169.254/latest/api/token\" -H \"X-aws-ec2-metadata-token-ttl-seconds: 21600\")",
            "    REGION=$(curl -s -H \"X-aws-ec2-metadata-token: $TOKEN\" http://169.254.169.254/latest/meta-data/placement/region)",
            "",
            "    if [ -z \"$REGION\" ]; then",
            "        log \"Failed to determine region from instance metadata\"",
            "        exit 1",
            "    else",
            "        log \"Detected region: $REGION\"",
            "    fi",
            "",
            "    # Configure AWS CLI to use instance metadata for credentials",
            "    mkdir -p ~/.aws",
            "    cat > ~/.aws/config << EOF",
            "[default]",
            "region = $REGION",
            "output = json",
            "EOF",
            "",
            "    log \"AWS CLI configured successfully for region: $REGION\"",
            "}",
            "",
            "# Function to read log archive path from file",
            "read_log_archive_path() {",
            "    if [ ! -f \"$LOG_PATHS_FILE\" ]; then",
            "        log \"Log paths file not found: $LOG_PATHS_FILE\" >&2",
            "        echo \"{\\\"Output\\\": \\\"Failed to upload logs to S3. Log paths file not found: $LOG_PATHS_FILE\\\"}\" >&2",
            "        exit 1",
            "    fi",
            "",
            "    # Read the first non-empty, non-comment line",
            "    local log_archive_path=\"\"",
            "    while IFS= read -r line; do",
            "        # Skip empty lines and comments",
            "        if [[ -n \"$line\" && ! \"$line\" =~ ^[[:space:]]*# ]]; then",
            "            # Remove leading/trailing whitespace",
            "            line=$(echo \"$line\" | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')",
            "            if [ -n \"$line\" ]; then",
            "                log_archive_path=\"$line\"",
            "                break",
            "            fi",
            "        fi",
            "    done < \"$LOG_PATHS_FILE\"",
            "",
            "    if [ -z \"$log_archive_path\" ]; then",
            "        log \"No valid log archive path found in $LOG_PATHS_FILE\" >&2",
            "        echo \"{\\\"Output\\\": \\\"Failed to upload logs to S3. No valid log archive path found.\\\"}\" >&2",
            "        exit 1",
            "    fi",
            "",
            "    # Verify the archive file exists",
            "    if [ ! -f \"$log_archive_path\" ]; then",
            "        log \"Log archive file not found: $log_archive_path\" >&2",
            "        echo \"{\\\"Output\\\": \\\"Failed to upload logs to S3. Log archive file not found: $log_archive_path\\\"}\" >&2",
            "        exit 1",
            "    fi",
            "",
            "    # Return only the clean path to stdout",
            "    echo \"$log_archive_path\"",
            "}",
            "",
            "# Clean S3 prefix by removing leading/trailing forward slash",
            "clean_s3_prefix() {",
            "    local prefix=\"$1\"",
            "",
            "    # Remove leading slashes",
            "    prefix=\"${prefix#/}\"",
            "",
            "    # Remove trailing slashes",
            "    prefix=\"${prefix%/}\"",
            "",
            "    echo \"$prefix\"",
            "}",
            "",
            "# Function to upload logs to S3",
            "upload_logs_to_s3() {",
            "    local s3_bucket=\"$1\"",
            "    local s3_prefix",
            "    s3_prefix=$(clean_s3_prefix \"$2\")",
            "    local bucket_owner_account=\"$3\"",
            "    local instance_id=\"$4\"",
            "    local filename_prefix=\"ec2-linux-logs\"",
            "",
            "    if [ -z \"$s3_bucket\" ]; then",
            "        log \"No S3 bucket specified, logs will remain on local storage\"",
            "        echo \"{\\\"Output\\\": \\\"Logs collection completed. No S3 bucket specified, logs stored locally.\\\"}\"",
            "        return 0",
            "    fi",
            "",
            "    log \"Uploading logs to S3 bucket: $s3_bucket\"",
            "",
            "    # Read the pre-compressed log archive path",
            "    local log_archive_path",
            "    log_archive_path=$(read_log_archive_path)",
            "",
            "    # Create a timestamp for the S3 object name",
            "    TIMESTAMP=$(date +\"%Y%m%d-%H%M%S\")",
            "",
            "    # Extract original filename without path",
            "    local original_filename",
            "    original_filename=$(basename \"$log_archive_path\")",
            "",
            "    # Create S3 object name with timestamp and instance ID",
            "    local s3_object_name=\"${filename_prefix}-${instance_id}-${TIMESTAMP}-${original_filename}\"",
            "",
            "    # Construct S3 destination path",
            "    local s3_destination=\"s3://$s3_bucket/${s3_prefix}/$instance_id/$s3_object_name\"",
            "    local s3_key=\"${s3_prefix}/$instance_id/$s3_object_name\"",
            "",
            "    log \"Uploading archive to: $s3_destination\"",
            "    log \"Using existing archive: $log_archive_path\"",
            "",
            "    # Execute upload using the pre-compressed archive",
            "    if [ -n \"$bucket_owner_account\" ]; then",
            "        aws s3api put-object \\",
            "            --bucket \"$s3_bucket\" \\",
            "            --key \"$s3_key\" \\",
            "            --body \"$log_archive_path\" \\",
            "            --acl bucket-owner-full-control \\",
            "            --expected-bucket-owner \"$bucket_owner_account\" >/dev/null",
            "        upload_result=$?",
            "    else",
            "        log \"Unable to determine bucket owner account, upload failed\"",
            "        echo \"{\\\"Output\\\": \\\"Failed to upload logs to S3. Unable to determine bucket owner account.\\\"}\"",
            "        exit 1",
            "    fi",
            "",
            "    # Verify upload succeeded before cleanup",
            "    if [ $upload_result -eq 0 ]; then",
            "        log \"Successfully uploaded logs to S3: $s3_destination\"",
            "        ",
            "        # Clean up log archive after successful upload",
            "        if [ -f \"$log_archive_path\" ]; then",
            "            log \"Cleaning up log archives\"",
            "            rm -rf \"$LOG_DIR\"",
            "            cleanup_result=$?",
            "",
            "            if [ $cleanup_result -eq 0 ]; then",
            "                log \"Log directory: $LOG_DIR cleaned up successfully\"",
            "            else",
            "                log \"Warning: Failed to remove log directory: $LOG_DIR. You can manually remove by running rm -rf $LOG_DIR on the instance\"",
            "            fi",
            "        fi",
            "    else",
            "        log \"Failed to upload logs to S3\"",
            "        echo \"{\\\"Output\\\": \\\"Failed to upload logs to S3\\\"}\"",
            "        exit 1",
            "    fi",
            "}",
            "",
            "# Function to retrieve BucketOwnerAccount",
            "get_bucket_owner_account() {",
            "    local bucket_owner_account=\"\"",
            "    bucket_owner_account=\"{{ global:ACCOUNT_ID }}\"    ",
            "    echo \"$bucket_owner_account\"",
            "}",
            "",
            "# Main function",
            "main() {",
            "    log \"Starting log upload process\"",
            "",
            "    # Parse input parameters from environment variables set by SSM",
            "    # These correspond to the SSM automation parameters",
            "    S3_LOG_DESTINATION=\"{{ S3LogDestination }}\"",
            "    S3_PREFIX=\"{{ S3Prefix }}\"",
            "    BUCKET_OWNER_ACCOUNT=$(get_bucket_owner_account)",
            "    INSTALL_AWS_CLI=\"{{ InstallAWSCLI }}\"",
            "",
            "    log \"Parameters received:\"",
            "    log \"  LogDestination: ${S3_LOG_DESTINATION:-'(empty - logs will be stored locally)'}\"",
            "    log \"  S3Prefix: $S3_PREFIX\"",
            "    log \"  InstallAWSCLI: $INSTALL_AWS_CLI\"",
            "",
            "    # Get instance ID from metadata service",
            "    TOKEN=$(curl -s -X PUT \"http://169.254.169.254/latest/api/token\" -H \"X-aws-ec2-metadata-token-ttl-seconds: 21600\")",
            "    INSTANCE_ID=$(curl -s -H \"X-aws-ec2-metadata-token: $TOKEN\" http://169.254.169.254/latest/meta-data/instance-id)",
            "",
            "    if [ -z \"$INSTANCE_ID\" ]; then",
            "        log \"Failed to get instance ID from metadata service\"",
            "        INSTANCE_ID=\"unknown-instance\"",
            "    fi",
            "",
            "    log \"Instance ID: $INSTANCE_ID\"",
            "",
            "    # Only proceed with AWS CLI setup if S3 destination is specified",
            "    if [ -n \"$S3_LOG_DESTINATION\" ]; then",
            "        # Check if AWS CLI is installed",
            "        if ! check_aws_cli; then",
            "            if [ \"${INSTALL_AWS_CLI}\" == \"Yes\" ]; then",
            "                install_aws_cli",
            "            else",
            "                log \"AWS CLI not installed and installation not requested\"",
            "                echo \"{\\\"Output\\\": \\\"Failed to upload logs to S3. AWS CLI not installed and installation not requested.\\\"}\"",
            "                exit 1",
            "            fi",
            "        fi",
            "",
            "        # Configure AWS CLI",
            "        configure_aws_cli",
            "    fi",
            "",
            "    # Upload logs to S3 (or handle local storage)",
            "    upload_logs_to_s3 \"$S3_LOG_DESTINATION\" \"$S3_PREFIX\" \"$BUCKET_OWNER_ACCOUNT\" \"$INSTANCE_ID\"",
            "}",
            "",
            "# Execute main function",
            "main \"$@\""
          ]
        },
        "CloudWatchOutputConfig": {
          "CloudWatchOutputEnabled": true
        }
      },
      "outputs": [
        {
          "Name": "UploadStatus",
          "Selector": "$.Output",
          "Type": "String"
        }
      ],
      "timeoutSeconds": 3600,
      "nextStep": "GenerateReport"
    },
    {
      "name": "GenerateReport",
      "description": "Generates a report of the log collection process. If Amazon S3 bucket was provided, it includes the Amazon S3 bucket name and prefix where logs were uploaded. If not, it indicates that logs were stored locally on the instance. It also reports why any previous steps failed.",
      "action": "aws:executeScript",
      "isEnd": true,
      "inputs": {
        "InputPayload": {
          "S3LogDestination": "{{ S3LogDestination }}",
          "S3Prefix": "{{ S3Prefix }}",
          "PlatformName": "{{ GetInstanceInformation.PlatformName }}",
          "CollectLogsOutput": "{{ CollectLogs.LogOutput }}",
          "BucketOwnerAccount": "{{ CheckS3BucketPublicStatus.bucketOwner }}",
          "UploadStatus": "{{ UploadLogsToS3.UploadStatus }}"
        },
        "Handler": "generate_report.lambda_handler",
        "Runtime": "python3.11",
        "Attachment": "artifact_2.zip"
      },
      "outputs": [
        {
          "Name": "Summary",
          "Selector": "$.Payload.Summary",
          "Type": "String"
        },
        {
          "Name": "LogLocation",
          "Selector": "$.Payload.LogLocation",
          "Type": "String"
        },
        {
          "Name": "Status",
          "Selector": "$.Payload.Status",
          "Type": "String"
        }
      ],
      "onFailure": "Abort",
      "timeoutSeconds": 600,
      "maxAttempts": 1
    }
  ],
  "outputs": [
    "GenerateReport.Summary",
    "GenerateReport.LogLocation",
    "GenerateReport.Status"
  ],
  "files": {
    "artifact_1.zip": {
      "checksums": {
        "SHA256": "c23dc4ed3063bc6e7410641233eb48efaa7be6205ab89816c24635834cc75a53"
      }
    },
    "artifact_2.zip": {
      "checksums": {
        "SHA256": "65114a3fc8399ac0a69cbf898badeba8262013cd8f9c680d5bb6b62b8c82a220"
      }
    }
  }
}
