{
  "schemaVersion": "0.3",
  "description": "The **AWSSupport-SendLogBundleToS3Bucket** runbook uploads a log bundle generated by the EC2Rescue tool for [Windows](https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/Windows-Server-EC2Rescue.html) or [Linux](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Linux-Server-EC2Rescue.html) from the target instance to the specified Amazon Simple Storage Service (Amazon S3). The runbook installs the platform specific version of EC2Rescue based on the instance's platform `Linux` or `Windows`. EC2Rescue is then used to collect the available operating system (OS) logs.\n\n### Prerequisites:\n> * Windows PowerShell 3.0 or later must be installed on Windows instances.\n> * [AWS Tools for PowerShell](https://aws.amazon.com/powershell/) must be installed on Windows instances.\n> * The [AWS Command Line Interface (AWS CLI)](https://aws.amazon.com/cli/) installed and configured on Linux instances.\n> * Write access to the specified Amazon S3 bucket.\n> * The instance subnet must have outbound connectivity to Amazon S3.\n> * The instance requires a valid AWS Identity and Access Management (IAM) instance profile that provides permissions for Systems Manager and the Amazon S3 bucket.\n\n### Important:\n> * As a best practice, always verify that the associated bucket policy and ACLs do not grant any unnecessary read or write permissions to principals that do not need access the bucket.\n> * We recommend setting Amazon S3 [server-side encryption](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingEncryption.html). You can also enable [Amazon S3 server access logging](https://docs.aws.amazon.com/AmazonS3/latest/userguide/enable-server-access-logging.html) to log the requests that are made to the bucket, and [Amazon S3 Versioning](https://docs.aws.amazon.com/AmazonS3/latest/userguide/manage-versioning-examples.html) to keep multiple versions of your objects.",
  "assumeRole": "{{ AutomationAssumeRole }}",
  "parameters": {
    "AutomationAssumeRole": {
      "type": "AWS::IAM::Role::Arn",
      "description": "(Optional) The Amazon Resource Name (ARN) of the AWS Identity and Access Management (IAM) role that allows Systems Manager Automation to perform the actions on your behalf. If no role is specified, Systems Manager Automation uses the permissions of the user that starts this runbook.",
      "default": ""
    },
    "InstanceId": {
      "type": "AWS::EC2::Instance::Id",
      "description": "(Required) The ID of the Windows or Linux managed instance you want to collect logs from."
    },
    "S3BucketName": {
      "type": "AWS::S3::Bucket::Name",
      "description": "(Required) The Amazon S3 bucket to upload the logs to. Please make sure the bucket is configured with server-side encryption (SSE), and the bucket policy does not grant unnecessary read/write permissions to parties that do not need to access the logs. Also please make sure EC2 Windows instance has necessary access to the S3 Bucket."
    },
    "S3Path": {
      "type": "String",
      "description": "(Optional) The Amazon S3 path for the collected logs. By default: `AWSSupport-SendLogBundleToS3Bucket/`.",
      "default": "AWSSupport-SendLogBundleToS3Bucket/",
      "allowedPattern": "^[a-zA-Z0-9][-./a-zA-Z0-9]{0,255}$"
    },
    "S3BucketOwnerRoleArn": {
      "type": "AWS::IAM::Role::Arn",
      "description": "(Optional) The ARN of the IAM role with permissions to get the Amazon S3 bucket and account block public access settings, bucket encryption configuration, the bucket ACLs, the bucket policy status, and upload objects to the bucket. If this parameter is not specified, the runbook uses the `AutomationAssumeRole` (if specified) or user that starts this runbook (if `AutomationAssumeRole` is not specified). Please see the required permissions section in the runbook description.",
      "default": ""
    },
    "S3BucketOwnerAccount": {
      "type": "String",
      "description": "(Optional) The AWS account that owns the Amazon S3 bucket. If you do not specify this parameter, the runbook assumes that the bucket is in this account.",
      "allowedPattern": "^$|^\\{\\{ global:ACCOUNT_ID \\}\\}$|^[0-9]{12}$",
      "default": "{{ global:ACCOUNT_ID }}"
    }
  },
  "mainSteps": [
    {
      "name": "checkBucketPublicStatus",
      "action": "aws:executeScript",
      "description": "Checks if the target Amazon S3 bucket potentially grants **read** and/or **write** `public` access to its objects.",
      "inputs": {
        "InputPayload": {
          "Bucket": "{{ S3BucketName }}",
          "BucketOwnerRoleArn": "{{ S3BucketOwnerRoleArn }}",
          "BucketOwnerAccount": "{{ S3BucketOwnerAccount }}",
          "AutomationAssumeRole": "{{ AutomationAssumeRole }}"
        },
        "Handler": "s3_bucket_public_status.check_bucket_public_status",
        "Runtime": "python3.8",
        "Attachment": "check_bucket_public_status.zip"
      },
      "outputs": [
        {
          "Name": "BucketLocation",
          "Selector": "$.Payload.location",
          "Type": "String"
        }
      ],
      "onFailure": "Abort",
      "isCritical": true,
      "nextStep": "assertInstanceIsManagedInstance"
    },
    {
      "name": "assertInstanceIsManagedInstance",
      "action": "aws:assertAwsResourceProperty",
      "description": "Ensures the target EC2 instance is managed by AWS Systems Manager, otherwise the automation ends.",
      "onFailure": "Abort",
      "inputs": {
        "Service": "ssm",
        "Api": "DescribeInstanceInformation",
        "InstanceInformationFilterList": [
          {
            "key": "InstanceIds",
            "valueSet": [
              "{{ InstanceId }}"
            ]
          }
        ],
        "PropertySelector": "$.InstanceInformationList[0].PingStatus",
        "DesiredValues": [
          "Online"
        ]
      },
      "isCritical": true,
      "nextStep": "describeManagedInstance"
    },
    {
      "name": "describeManagedInstance",
      "action": "aws:executeAwsApi",
      "description": "Gets information about the operating system (OS) platform of the instance specified in the `InstanceId` parameter.",
      "onFailure": "Abort",
      "inputs": {
        "Service": "ssm",
        "Api": "DescribeInstanceInformation",
        "InstanceInformationFilterList": [
          {
            "key": "InstanceIds",
            "valueSet": [
              "{{ InstanceId }}"
            ]
          }
        ]
      },
      "outputs": [
        {
          "Name": "Platform",
          "Selector": "$.InstanceInformationList[0].PlatformType"
        }
      ],
      "isCritical": true,
      "nextStep": "branchOnManagedInstancePlatform"
    },
    {
      "name": "branchOnManagedInstancePlatform",
      "action": "aws:branch",
      "description": "Branches the automation based on the OS platform `Linux` or `Windows`.",
      "onFailure": "Abort",
      "inputs": {
        "Choices": [
          {
            "NextStep": "installEC2RescueForWindows",
            "Variable": "{{ describeManagedInstance.Platform }}",
            "StringEquals": "Windows"
          },
          {
            "NextStep": "installEC2RescueForLinux",
            "Variable": "{{ describeManagedInstance.Platform }}",
            "StringEquals": "Linux"
          }
        ]
      },
      "isCritical": true,
      "isEnd": true
    },
    {
      "name": "installEC2RescueForWindows",
      "action": "aws:runCommand",
      "description": "Installs EC2Rescue for Windows via `AWS-ConfigureAWSPackage`.",
      "onFailure": "Abort",
      "inputs": {
        "DocumentName": "AWS-ConfigureAWSPackage",
        "InstanceIds": [
          "{{ InstanceId }}"
        ],
        "Parameters": {
          "name": "AWSSupport-EC2Rescue",
          "action": "Install",
          "version": "latest"
        }
      },
      "isCritical": true,
      "nextStep": "collectAndUploadWindowsLogBundle"
    },
    {
      "name": "collectAndUploadWindowsLogBundle",
      "action": "aws:runCommand",
      "description": "Runs the PowerShell script to collect Windows troubleshooting logs with EC2Rescue and upload the logs to the Amazon S3 bucket.",
      "onFailure": "Abort",
      "inputs": {
        "DocumentName": "AWS-RunPowerShellScript",
        "InstanceIds": [
          "{{ InstanceId }}"
        ],
        "Parameters": {
          "commands": [
            "# Copyright 2024 Amazon.com, Inc. or its affiliates. All Rights Reserved.",
            "# SPDX-License-Identifier: LicenseRef-.amazon.com.-AmznSL-1.0",
            "# Licensed under the Amazon Software License http://aws.amazon.com/asl/",
            "",
            "# Set AWS Tools for PowerShell environment variables",
            "$Env:AWS_RETRY_MODE=\"standard\" # Specifies the retry mode for AWS API calls",
            "$Env:AWS_MAX_ATTEMPTS=10 # Specifies the maximum retry attempts",
            "$Env:AWS_DEFAULT_REGION=\"{{global:REGION}}\" # Sets default region",
            "",
            "try {",
            "    # Check if AWSPowershell module is available",
            "    if (Get-Module -ListAvailable -Name AWSPowershell) {",
            "        # Import necessary modules",
            "        Import-Module AWSPowershell, EC2Rescue",
            "        [System.Net.ServicePointManager]::SecurityProtocol = ([int][system.net.SecurityProtocolType]::Tls13 -bor [int][system.net.SecurityProtocolType]::Tls12)",
            "",
            "        # Get S3 bucket details from template or environment variables",
            "        $s3BucketName = \"{{ S3BucketName }}\"",
            "        $s3Path = \"{{ S3Path }}\".TrimEnd(\"/\")",
            "        # Add \"/\" to the end of the {{ S3Path }}",
            "        $s3Path = \"{0}/\" -f $s3Path",
            "",
            "        # Determine the region of the current EC2 instance",
            "        $instanceRegion = \"{{ global:REGION }}\"",
            "        $accountId = \"{{ global:ACCOUNT_ID }}\"",
            "",
            "        # Set the region of the specified S3 bucket",
            "        $s3BucketRegion = \"{{ checkBucketPublicStatus.BucketLocation }}\"",
            "",
            "        # Check if the specified S3 bucket exists",
            "        if (Test-S3Bucket -BucketName $s3BucketName -Region $s3BucketRegion) {",
            "            # Collect logs from the current Windows instance",
            "            Write-Output \"Collecting logs from the current Windows instance.\"",
            "            $logFile = Invoke-EC2RescueCollectLogBundle -Logs \"all\"",
            "",
            "            # Upload logs to the specified S3 bucket and path",
            "            Write-Output \"Log collection completed. Uploading logs to S3 bucket ${s3BucketName} under path ${s3Path}.\"",
            "            Copy-EC2RescueLogBundleToS3 -FilePath $logFile -S3BucketName $s3BucketName -S3Path $s3Path",
            "            Write-Output \"Log upload completed.\"",
            "        } else {",
            "            # Throw an exception if the specified S3 bucket is not found or access is denied",
            "            throw (\"No Amazon S3 bucket called \" + $s3BucketName + \" found in the current AWS account, or access denied. Please specify an Amazon S3 bucket you own, and that this instance has access to.\")",
            "        }",
            "    } else {",
            "        # Inform the user that AWS Tools for Windows PowerShell is not installed",
            "        Write-Output \"AWS Tools for Windows PowerShell not installed. Please install the latest version of the AWS Tools for Windows PowerShell and try again.\"",
            "        Write-Output \"Download location: https://aws.amazon.com/powershell/\"",
            "",
            "        # Exit with code 255 (indicating an error)",
            "        Exit 255",
            "    }",
            "} catch {",
            "    # Catch and handle any exceptions during execution",
            "    Write-Output $_.Exception.Message",
            "",
            "    # Exit with code 1 (indicating an error)",
            "    Exit 1",
            "}"
          ]
        }
      },
      "isCritical": true,
      "isEnd": true
    },
    {
      "name": "installEC2RescueForLinux",
      "action": "aws:runCommand",
      "description": "Installs EC2Rescue for Linux via `AWS-ConfigureAWSPackage`.",
      "onFailure": "Abort",
      "inputs": {
        "DocumentName": "AWS-ConfigureAWSPackage",
        "InstanceIds": [
          "{{ InstanceId }}"
        ],
        "Parameters": {
          "name": "AWSSupport-EC2Rescue",
          "action": "Install",
          "version": "latest"
        }
      },
      "isCritical": true,
      "nextStep": "collectAndUploadLinuxLogBundle"
    },
    {
      "name": "collectAndUploadLinuxLogBundle",
      "action": "aws:runCommand",
      "description": "Runs the bash script to collect Linux troubleshooting logs with EC2Rescue and upload the logs to the Amazon S3 bucket.",
      "onFailure": "Abort",
      "inputs": {
        "DocumentName": "AWS-RunShellScript",
        "InstanceIds": [
          "{{ InstanceId }}"
        ],
        "Parameters": {
          "commands": [
            "#!/bin/bash",
            "",
            "# Copyright 2024 Amazon.com, Inc. or its affiliates. All Rights Reserved.",
            "# SPDX-License-Identifier: LicenseRef-.amazon.com.-AmznSL-1.0",
            "# Licensed under the Amazon Software License  http://aws.amazon.com/asl/",
            "",
            "error_trap() {",
            "    if [[ -n \"$1\" ]]; then",
            "        printf \"Error: %s\\n\" \"$1\" >&2",
            "    fi",
            "    printf \"Error: The Execution did not complete successfully.\" >&2",
            "    exit 1",
            "}",
            "",
            "INSTANCE_ID=\"{{InstanceId}}\"",
            "BUCKET=\"{{S3BucketName}}\"",
            "EXECUTION_ID=\"{{automation:EXECUTION_ID}}\"",
            "# Get the Amazon S3 bucket path without the trailing /",
            "BUCKET_PATH=$(echo \"{{S3Path}}\" | sed 's#/\\+$##')",
            "ACCOUNT_ID=\"{{global:ACCOUNT_ID}}\"",
            "",
            "# Set AWS CLI environment variables",
            "export AWS_RETRY_MODE=\"standard\" # Specifies the retry mode for AWS API calls",
            "export AWS_MAX_ATTEMPTS=10 # Specifies the maximum retry attempts",
            "export AWS_DEFAULT_REGION=\"{{global:REGION}}\" # Sets default region",
            "",
            "# Set the Amazon S3 bucket location",
            "BUCKET_REGION=\"{{checkBucketPublicStatus.BucketLocation}}\"",
            "",
            "# Create a unique tarball filename using EXECUTION_ID and INSTANCE_ID",
            "FILE_NAME=\"${EXECUTION_ID}_${INSTANCE_ID}.tgz\"",
            "",
            "# Make sure the bucket is accessible before running ec2rl",
            "aws s3api head-bucket --bucket \"$BUCKET\" --region \"$BUCKET_REGION\" --expected-bucket-owner \"$ACCOUNT_ID\" 1> /dev/null || error_trap \"No Amazon S3 bucket called $BUCKET found in the current AWS account or access denied. Please specify an Amazon S3 bucket you own and that this instance has access to.\"",
            "",
            "# Determine if the system is an instance",
            "NOT_AN_INSTANCE=\"--not-an-instance\"",
            "if ( [ -f /sys/hypervisor/uuid ] && grep -iE \"^ec2\" /sys/hypervisor/uuid >/dev/null ) \\",
            "|| ( [ -f /sys/devices/virtual/dmi/id/product_uuid ] && grep -iE \"^ec2\" /sys/devices/virtual/dmi/id/product_uuid >/dev/null ); then",
            "NOT_AN_INSTANCE=\"\"",
            "fi",
            "",
            "printf \"Running EC2 Rescue for Linux\\n\"",
            "printf \"The ec2rl run output directory will be uploaded to s3://%s/%s/%s\\n\" \"$BUCKET\" \"$BUCKET_PATH\" \"$FILE_NAME\"",
            "sudo ec2rl run ${NOT_AN_INSTANCE} --only-classes=collect,gather --times=3 --period=5 --domain=amazon.com --protocol=tcp --count=3 --destination=amazon.com --port=80 || error_trap \"The EC2Rescue execution did not complete successfully.\"",
            "",
            "# Find the most recently modified directory in /var/tmp/ec2rl",
            "LOG_DIR=\"$(basename \"$(find /var/tmp/ec2rl -maxdepth 1 -mindepth 1 -printf \"%T+ %p\\\\n\" | sort -r | head -n 1 | awk '{print $2}')\")\"",
            "",
            "printf \"Creating tarball %s.tgz of EC2RL log directory /var/tmp/ec2rl/%s\\n\" \"$EXECUTION_ID\" \"$LOG_DIR\"",
            "# Create a tarball of the ec2rl log directory",
            "tar -czf \"${FILE_NAME}\" -C /var/tmp/ec2rl \"${LOG_DIR}\" || error_trap \"Error compressing ${LOG_DIR}\"",
            "",
            "printf \"Uploading tarball to s3://%s/%s/%s\\n\" \"$BUCKET\" \"$BUCKET_PATH\" \"$FILE_NAME\"",
            "",
            "# Upload the tarball to the specified S3 bucket and path",
            "aws s3 cp \"$FILE_NAME\" \"s3://$BUCKET/$BUCKET_PATH/$FILE_NAME\" --region \"$BUCKET_REGION\" || error_trap \"Error uploading file to s3://$BUCKET/$BUCKET_PATH/$FILE_NAME\"",
            "printf \"Done\""
          ]
        }
      },
      "isCritical": true,
      "isEnd": true
    }
  ],
  "outputs": [
    "collectAndUploadWindowsLogBundle.Output",
    "collectAndUploadLinuxLogBundle.Output"
  ],
  "files": {
    "check_bucket_public_status.zip": {
      "checksums": {
        "SHA256": "be0dee25ffdfeedf7e18bce29ddb6093ea8999b87510499d8e141fcf2c9d721b"
      }
    }
  }
}
