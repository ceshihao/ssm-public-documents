{
  "description": "The **AWSSupport-TroubleshootS3PublicRead** helps diagnose Amazon Simple Storage Service (Amazon S3) public objects read access issues by analyzing the S3 bucket and a limited number of objects configuration including: the account and bucket 'block public access' settings, the bucket policy, objects ACL, objects server side encryption, etc.",
  "schemaVersion": "0.3",
  "assumeRole": "{{AutomationAssumeRole}}",
  "parameters": {
    "AutomationAssumeRole": {
      "type": "String",
      "description": "(Optional) The ARN of the role that allows Automation to perform the actions on your behalf. If no role is specified, Systems Manager Automation uses your IAM permissions to execute this document.",
      "default": ""
    },
    "S3BucketName": {
      "type": "String",
      "description": "(Required) Specify the name of your Amazon S3 bucket.",
      "allowedPattern": "^[a-zA-Z0-9\\.\\-_\\\\]{1,255}$",
      "maxChars": 255
    },
    "S3PrefixName": {
      "type": "String",
      "description": "(Optional) Specify the prefix or name of the object key(s) residing in your Amazon S3 bucket. E.g: keyname, key*, level1/, or level1/keyname.",
      "default": "",
      "allowedPattern": "^[a-zA-Z0-9\\.\\-_\\\\!*'()/]{0,1024}$",
      "maxChars": 1024
    },
    "StartAfter": {
      "type": "String",
      "description": "(Optional) StartAfter is the key name where you want the document to start listing from. StartAfter can be any key in the bucket.",
      "default": "",
      "allowedPattern": "^[a-zA-Z0-9\\.\\-_\\\\!*'()/]{0,1024}$",
      "maxChars": 1024
    },
    "MaxObjects": {
      "type": "Integer",
      "description": "Maximum number of objects returned for analysis (between 1 and 25).",
      "allowedPattern": "^[1-9]|0[1-9]|1[0-9]|2[0-5]$",
      "default": 5,
      "allowedValues": [
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        8,
        9,
        10,
        11,
        12,
        13,
        14,
        15,
        16,
        17,
        18,
        19,
        20,
        21,
        22,
        23,
        24,
        25
      ]
    },
    "IgnoreBlockPublicAccess": {
      "type": "Boolean",
      "description": "Specify if you want to ignore the account and bucket block public access settings. Changing this option is not recommended. Changing this option to 'true', causes the document analysis to not consider public access settings that might be blocking public read access to your objects.",
      "default": false,
      "allowedValues": [
        false,
        true
      ]
    },
    "HttpGet": {
      "type": "Boolean",
      "description": "Specify if you want the automation document to perform a partial HTTP GET request of the object. The document only retrieves the first 100 bytes using the Range HTTP header.",
      "default": true,
      "allowedValues": [
        true,
        false
      ]
    },
    "Verbose": {
      "type": "Boolean",
      "description": "Specify if you want to see detailed information during the analysis or only warning and error messages.",
      "default": false,
      "allowedValues": [
        false,
        true
      ]
    },
    "CloudWatchLogGroupName": {
      "type": "String",
      "description": "(Optional) CloudWatch Log Group Name you want to send the analysis result and log data. If you specify a name and it does not exist, the SSM Automation document will try to create it on your behalf.",
      "default": "",
      "allowedPattern": "^[\\.\\-_/#A-Za-z0-9]{0,512}$",
      "maxChars": 512
    },
    "CloudWatchLogStreamName": {
      "type": "String",
      "description": "(Optional) CloudWatch Log Stream Name you want to send the analysis result and log data. If does not exist, the SSM Automation document will try to create it on your behalf. If you leave this input parameter empty, the document will use the SSM Automation execution Id as the name.",
      "default": "",
      "allowedPattern": "^[\\.\\-_/#A-Za-z0-9]{0,512}$",
      "maxChars": 512
    },
    "ResourcePartition": {
      "type": "String",
      "description": "(Required) The partition in which the S3 bucket is located. The partition is used for the bucket policy simulation.",
      "default": "aws",
      "allowedValues": [
        "aws",
        "aws-us-gov",
        "aws-cn"
      ]
    }
  },
  "mainSteps": [
    {
      "name": "TestBucketAccess",
      "action": "aws:assertAwsResourceProperty",
      "description": "test description",
      "onFailure": "Abort",
      "inputs": {
        "Service": "s3",
        "Api": "HeadBucket",
        "Bucket": "{{S3BucketName}}",
        "PropertySelector": "$.ResponseMetadata.HTTPStatusCode",
        "DesiredValues": [
          "200"
        ]
      }
    },
    {
      "name": "GetBucketInformation",
      "action": "aws:executeScript",
      "timeoutSeconds": 180,
      "onFailure": "Abort",
      "inputs": {
        "Runtime": "python3.6",
        "Handler": "get_bucket_information",
        "InputPayload": {
          "S3BucketName": "{{S3BucketName}}"
        },
        "Script": "# Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this\n# software and associated documentation files (the \"Software\"), to deal in the Software\n# without restriction, including without limitation the rights to use, copy, modify,\n# merge, publish, distribute, sublicense, and/or sell copies of the Software, and to\n# permit persons to whom the Software is furnished to do so.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,\n# INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A\n# PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT\n# HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\n# OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n# SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport sys\nimport boto3\nimport json\nimport logging\nfrom botocore.errorfactory import ClientError\n\nsys.tracebacklimit=0\nclient = boto3.client('s3')\n\ndef get_bucket_location(bucket_name):\n    try:\n        location = client.get_bucket_location(Bucket=bucket_name).get('LocationConstraint',{})\n        if location is None:\n            location = 'us-east-1'\n        elif location == 'EU':\n            location = 'eu-west-1'\n        return location\n    \n    except ClientError as e:\n        print(f'Error: {ex.response}')\n \ndef get_account_canonical_id():\n    try:\n        return client.list_buckets().get('Owner', {}).get('ID')\n    \n    except ClientError as e:\n        print(f'Error: {ex.response}')\n\ndef get_bucket_information(events,context):\n    \"\"\"\n    Description:\n        Calls s3.GetBucketLocation to retrieve the bucket location\n        Calls s3.ListBuckets (--query Owner.ID) to get the Amazon S3 canonical user id\n    \n    Permission Required:\n        - 's3:GetBucketLocation'\n        - 's3:ListBuckets'\n    \n    Arguments:\n        events.bucket_name (str): Name of the S3 bucket\n        \n    Returns:\n        bucket_location (str): S3 bucket location\n        canonical_id (str): S3 bucket owner canonical user id\n    \"\"\"\n    bucket_name = events['S3BucketName']\n\n    try:\n        return {\n            'bucket_location':get_bucket_location(bucket_name),\n            'canonical_id':get_account_canonical_id()\n        }\n                \n    except Exception as e:\n        print(f'Error: {str(e)}')\n        raise\n    \n"
      },
      "outputs": [
        {
          "Name": "error",
          "Selector": "$.Payload.error",
          "Type": "Boolean"
        },
        {
          "Name": "bucket_location",
          "Selector": "$.Payload.bucket_location",
          "Type": "String"
        },
        {
          "Name": "canonical_id",
          "Selector": "$.Payload.canonical_id",
          "Type": "String"
        }
      ]
    },
    {
      "name": "GetBlockPublicAccess",
      "action": "aws:executeScript",
      "timeoutSeconds": 180,
      "onFailure": "Continue",
      "inputs": {
        "Runtime": "python3.6",
        "Handler": "get_block_public_access",
        "InputPayload": {
          "AccountId": "{{global:ACCOUNT_ID}}",
          "S3BucketName": "{{S3BucketName}}",
          "IgnoreBlockPublicAccess": "{{IgnoreBlockPublicAccess}}"
        },
        "Script": "# Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this\n# software and associated documentation files (the \"Software\"), to deal in the Software\n# without restriction, including without limitation the rights to use, copy, modify,\n# merge, publish, distribute, sublicense, and/or sell copies of the Software, and to\n# permit persons to whom the Software is furnished to do so.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,\n# INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A\n# PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT\n# HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\n# OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n# SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport sys\nimport boto3\nimport json\nfrom botocore.errorfactory import ClientError\n\nsys.tracebacklimit=0\nclient = boto3.client('s3')\n\ndef _get_public_access_settings(account_id,bucket_name=None):\n    \"\"\"\n    Description:\n        Calls s3control.GetPublicAccessBlock and s3.GetPublicAccessBlock to retrieves the \n        PublicAccessBlock configuration for the Amazon S3 bucket and the AWS Account.\n        Reference: https://docs.aws.amazon.com/AmazonS3/latest/dev/access-control-block-public-access.html\n    \n    Permission Required:\n        - 's3:GetBucketPublicAccessBlock'\n    \n    Arguments:\n        bucket_name (str): Name of the S3 bucket\n        account_id (str): AWS Account Id\n    \"\"\"\n        \n    try:\n        if bucket_name is None:\n            settings = boto3.client('s3control').get_public_access_block(AccountId=account_id)\n        else:\n            settings = client.get_public_access_block(Bucket=bucket_name)\n        \n        return settings['PublicAccessBlockConfiguration']\n    \n    except ClientError as ex:\n        if ex.response['Error']['Code'] == 'NoSuchPublicAccessBlockConfiguration':\n            return {\n                'BlockPublicAcls': False,\n                'IgnorePublicAcls': False,\n                'BlockPublicPolicy': False,\n                'RestrictPublicBuckets': False\n            }\n        else:\n            print(f'Error: {ex.response}')\n            raise\n\ndef get_block_public_access(events,context):\n    \"\"\"\n    Description:\n        Retrieves the PublicAccessBlock configuration for an Amazon S3 bucket and the AWS Account.\n    \n    Permission Required:\n        - 's3:GetBucketPublicAccessBlock'\n    \n    Arguments:\n        events.bucket_name (str): Name of the S3 bucket\n        events.account_id (str): Caller AWS Account Id\n        events.ignore_block_public (bool): IgnoreBlockPublicAccess automation input parameter. Ignores Public Access Settings.\n    \n    Returns:\n        block_ignore_public_acl (bool): IgnorePublicAcls setting ON/OFF\n        block_ignore_public_policy (bool): RestrictPublicBuckets setting ON/OFF\n    \"\"\"\n    account_id = events['AccountId']\n    bucket_name = events['S3BucketName']\n    ignore_block_public = events['IgnoreBlockPublicAccess']\n    \n    block_ignore_public_acl = False\n    block_ignore_public_policy = False\n\n    try:\n        if ignore_block_public:\n            print(f'Warning: Automation document IgnoreBlockPublicAccess input parameter is set to \"{ignore_block_public}\"')\n            print('This option causes the document analysis to not consider public access settings that might be blocking public read access to your objects.')\n        \n        # Get Block public access settings for Account (account_id)\n        account_settings = _get_public_access_settings(account_id)\n        print(f'Block public access settings (Account): {account_settings}')\n        \n        if account_settings.get('IgnorePublicAcls'):\n            print(f'Warning: (Account) Block public access to buckets and objects granted through any access control lists (ACLs).')\n            block_ignore_public_acl = True\n        if account_settings.get('RestrictPublicBuckets'):\n            print('Warning: (Account) Block public and cross-account access to buckets and objects through any public bucket or access point policies.')\n            block_ignore_public_policy = True\n        \n        # Get Block public access settings for Bucket (bucket_name)\n        settings = _get_public_access_settings(account_id,bucket_name)\n        print(f'Block public access settings (Bucket): {settings}')\n        \n        if settings.get('IgnorePublicAcls'):\n            print('Warning: (Bucket) Block public access to buckets and objects granted through any access control lists (ACLs).')\n            block_ignore_public_acl = True\n        if settings.get('RestrictPublicBuckets'):\n            print('Warning: (Bucket) Block public and cross-account access to buckets and objects through any public bucket or access point policies.')\n            block_ignore_public_policy = True\n        \n        # If S3 block public access prevents public access and the document is not ignoring them, return and error\n        if block_ignore_public_acl and block_ignore_public_policy and not ignore_block_public:\n            raise Exception('Public access settings are configured to ignore both public ACLs and any public bucket policy.')\n                    \n        return { 'block_ignore_public_acl':block_ignore_public_acl,\n                 'block_ignore_public_policy':block_ignore_public_policy }\n    \n    except Exception as e:\n        print(f'Error: {str(e)}')\n        raise\n    \n"
      },
      "outputs": [
        {
          "Name": "block_ignore_public_acl",
          "Selector": "$.Payload.block_ignore_public_acl",
          "Type": "Boolean"
        },
        {
          "Name": "block_ignore_public_policy",
          "Selector": "$.Payload.block_ignore_public_policy",
          "Type": "Boolean"
        }
      ]
    },
    {
      "name": "CheckBucketPayer",
      "action": "aws:assertAwsResourceProperty",
      "onFailure": "Abort",
      "isCritical": false,
      "inputs": {
        "Service": "s3",
        "Api": "GetBucketRequestPayment",
        "Bucket": "{{S3BucketName}}",
        "PropertySelector": "$.Payer",
        "DesiredValues": [
          "BucketOwner"
        ]
      }
    },
    {
      "name": "GetBucketPolicyStatus",
      "action": "aws:executeScript",
      "timeoutSeconds": 180,
      "onFailure": "Abort",
      "inputs": {
        "Runtime": "python3.6",
        "Handler": "get_bucket_policy_status",
        "InputPayload": {
          "S3BucketName": "{{S3BucketName}}",
          "PublicAccessIgnoreACL": "{{GetBlockPublicAccess.block_ignore_public_acl}}"
        },
        "Script": "# Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this\n# software and associated documentation files (the \"Software\"), to deal in the Software\n# without restriction, including without limitation the rights to use, copy, modify,\n# merge, publish, distribute, sublicense, and/or sell copies of the Software, and to\n# permit persons to whom the Software is furnished to do so.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,\n# INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A\n# PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT\n# HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\n# OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n# SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport sys\nimport boto3\nimport json\nfrom botocore.errorfactory import ClientError\n\nsys.tracebacklimit=0\nclient = boto3.client('s3')\n\ndef get_bucket_policy_status(events,context):\n    \"\"\"\n    Description:\n        Calls s3.GetBucketPolicyStatus to retrieve the S3 policy status indicating whether \n        the bucket is public (True) or not (False). Returns None if the bucket does \n        not have a policy associated\n    \n    Permission Required:\n        - 's3:GetBucketPolicyStatus'\n    \n    Arguments:\n        events.bucket_name (str): Name of the S3 bucket\n        events.block_ignore_public_acl (bool): Block public access to buckets and objects granted through \n        any access control lists (ACLs) setting\n    \n    Returns:\n        status (bool): is public (True) or not (False). Returns None if the bucket does \n        not have a policy associated\n    \"\"\"\n    \n    bucket_name = events['S3BucketName']\n    block_ignore_public_acl = events['PublicAccessIgnoreACL']\n\n    try:\n        is_public = client.get_bucket_policy_status(Bucket=bucket_name)['PolicyStatus']['IsPublic']\n        \n        if is_public:\n            print('Info: Bucket policy is marked as public.')\n            return { 'status':True }\n        else:\n            if block_ignore_public_acl:\n                raise Exception('Bucket policy is marked as private and the public access settings are configured to ignore public ACLs.')\n            else:\n                print('Warning: Bucket policy is marked as private.')\n                return { 'status':False }\n    except ClientError as ex:\n        if ex.response[\"Error\"][\"Code\"] == \"NoSuchBucketPolicy\":\n            print('Info: Bucket has no policy associated.')\n            return { 'status':None }\n        else:\n            print(f'Error: {ex.response}')\n            raise\n\n    except Exception as e:\n        print(f'Error: {str(e)}')\n        raise\n    \n"
      },
      "outputs": [
        {
          "Name": "status",
          "Selector": "$.Payload.status",
          "Type": "Boolean"
        }
      ]
    },
    {
      "name": "GetBucketPolicy",
      "action": "aws:executeAwsApi",
      "onFailure": "step:GetBucketAcl",
      "isCritical": false,
      "inputs": {
        "Service": "s3",
        "Api": "GetBucketPolicy",
        "Bucket": "{{S3BucketName}}"
      },
      "outputs": [
        {
          "Name": "Policy",
          "Selector": "$.Policy",
          "Type": "String"
        }
      ]
    },
    {
      "name": "GetContextKeys",
      "action": "aws:executeAwsApi",
      "onFailure": "Continue",
      "isCritical": false,
      "inputs": {
        "Service": "iam",
        "Api": "GetContextKeysForCustomPolicy",
        "PolicyInputList": [
          "{{GetBucketPolicy.Policy}}"
        ]
      },
      "outputs": [
        {
          "Name": "ContextKeyNames",
          "Selector": "$.ContextKeyNames",
          "Type": "StringList"
        }
      ]
    },
    {
      "name": "SimulateBucketPolicy",
      "action": "aws:assertAwsResourceProperty",
      "timeoutSeconds": 180,
      "onFailure": "Continue",
      "isCritical": false,
      "inputs": {
        "Service": "iam",
        "Api": "SimulateCustomPolicy",
        "PolicyInputList": [
          "{{GetBucketPolicy.Policy}}"
        ],
        "ResourceArns": [
          "arn:{{ResourcePartition}}:s3:::{{S3BucketName}}/{{S3PrefixName}}*"
        ],
        "ActionNames": [
          "s3:GetObject"
        ],
        "PropertySelector": "$.EvaluationResults[0].EvalDecision",
        "DesiredValues": [
          "allowed",
          "implicitDeny"
        ]
      }
    },
    {
      "name": "GetBucketAcl",
      "action": "aws:executeAwsApi",
      "timeoutSeconds": 180,
      "onFailure": "Continue",
      "inputs": {
        "Service": "s3",
        "Api": "GetBucketAcl",
        "Bucket": "{{S3BucketName}}"
      },
      "outputs": [
        {
          "Name": "Owner",
          "Selector": "$.Owner.ID",
          "Type": "String"
        },
        {
          "Name": "Grants",
          "Selector": "$.Grants",
          "Type": "MapList"
        }
      ]
    },
    {
      "name": "CreateLogandStream",
      "action": "aws:executeScript",
      "timeoutSeconds": 180,
      "onFailure": "Continue",
      "inputs": {
        "Runtime": "python3.6",
        "Handler": "create_cloudwatch_log_stream",
        "InputPayload": {
          "CloudWatchLogGroupName": "{{CloudWatchLogGroupName}}",
          "CloudWatchLogStreamName": "{{CloudWatchLogStreamName}}",
          "ExecutionId": "{{automation:EXECUTION_ID}}"
        },
        "Script": "# Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this\n# software and associated documentation files (the \"Software\"), to deal in the Software\n# without restriction, including without limitation the rights to use, copy, modify,\n# merge, publish, distribute, sublicense, and/or sell copies of the Software, and to\n# permit persons to whom the Software is furnished to do so.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,\n# INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A\n# PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT\n# HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\n# OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n# SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport sys\nimport boto3\n\nfrom botocore.errorfactory import ClientError\n\nsys.tracebacklimit=0\nlogs = boto3.client('logs')\n\ndef _create_log_group_if_not_exists(log_group_name,retention_days):\n    \"\"\"\n    Description:\n    Creates the Log Group if a log_group_name is provided. If a new Log Group is created,\n    the function tries to set the Logs retention days to retention_days.\n    \n    Permission Required:\n    - 'logs:CreateLogGroup'\n    - 'logs:PutRetentionPolicy'\n    \n    Arguments:\n    log_group_name: Name of the Log Group\n    retention_days: The number of days to retain the log events in the specified log group.\n    \"\"\"\n    print(f'Info: Creating Log Group {log_group_name}.')\n    try:\n        if log_group_name:\n            logs.create_log_group(logGroupName=log_group_name)\n            \n            try:\n                logs.put_retention_policy(logGroupName=log_group_name,retentionInDays=retention_days)\n            except ClientError as ex:\n                print(f'Error: {ex.response}')\n            \n            return True\n        else:\n            return False\n    except ClientError as ex:\n        # If the Log Group exist, don't return an error\n        if ex.response['Error']['Code'] == 'ResourceAlreadyExistsException':\n            print(f'Warning: Log Group {log_group_name} already exists.')\n            return True\n        else:\n            print(f'Error: {ex.response}')\n            return False\n\ndef _create_log_stream_if_not_exists(log_stream_name,log_group_name):\n    \"\"\"\n    Description:\n        Creates the Log Stream if a log_group_name and log_stream_name are provided.\n    \n    Permission Required:\n        - 'logs:CreateLogStream'\n    \n    Arguments:\n        log_group_name: Name of the Log Group\n        log_stream_name: Name of the Log Stream\n    \"\"\"\n    print(f'Info: Creating Log Stream {log_stream_name}.')\n    try:\n        if log_stream_name and log_group_name:\n            logs.create_log_stream(logGroupName=log_group_name, logStreamName=log_stream_name)\n            return True\n        else:\n            return False\n    except ClientError as ex:\n        # If the Log Stream exist, don't return an error\n        if ex.response['Error']['Code'] == 'ResourceAlreadyExistsException':\n            print(f'Warning: Log Stream {log_stream_name} already exists.')\n            return True\n        else:\n            print(f'Error: {ex.response}')\n            return False\n        \ndef create_cloudwatch_log_stream(events,context):\n    \"\"\"\n    Description:\n        Calls logs.CreateLogGroup and logs.CreateLogStream to create the CloudWath Log Group and Stream\n        if requested in the document. If CloudWatchLogStreamName is not provided, the automation\n        execution id is used. The log retention days is set to RETENTION_DAYS days.\n    \n    Permission Required:\n        - 'logs:CreateLogGroup'\n        - 'logs:CreateLogStream'\n        - 'logs:PutRetentionPolicy'\n    \n    Arguments:\n        events.CloudWatchLogGroupName: Name of the CloudWatch Log Group\n        events.CloudWatchLogStreamName: Name of the CloudWatch Log Stream\n        events.ExecutionId: Automation execution id (automation:EXECUTION_ID)\n    \"\"\"\n    \n    log_group_name = events.get('CloudWatchLogGroupName','')\n    log_stream_name = events.get('CloudWatchLogStreamName','')\n    execution_id = events.get('ExecutionId','')\n    RETENTION_DAYS = 14\n    \n    # If the CloudWatchLogStreamName is empty, use the Automation Execution Id\n    if not log_stream_name:\n        log_stream_name = execution_id\n\n    if not log_group_name:\n        return { 'error':True }\n          \n    if log_group_name and log_stream_name and _create_log_group_if_not_exists(log_group_name,RETENTION_DAYS) and _create_log_stream_if_not_exists(log_stream_name,log_group_name):\n        return { 'error':False }\n\n    return { 'error':True }\n"
      },
      "outputs": [
        {
          "Name": "error",
          "Selector": "$.Payload.error",
          "Type": "Boolean"
        }
      ]
    },
    {
      "name": "AnalyzeObjects",
      "action": "aws:executeScript",
      "timeoutSeconds": 600,
      "onFailure": "Abort",
      "inputs": {
        "Runtime": "python3.6",
        "Handler": "analyze_objects",
        "InputPayload": {
          "AccountId": "{{global:ACCOUNT_ID}}",
          "S3BucketName": "{{S3BucketName}}",
          "S3PrefixName": "{{S3PrefixName}}",
          "StartAfter": "{{StartAfter}}",
          "GetBucketInformationLocation": "{{GetBucketInformation.bucket_location}}",
          "GetBucketInformationOwner": "{{GetBucketInformation.canonical_id}}",
          "MaxObjects": "{{MaxObjects}}",
          "IgnoreBlockPublicAccess": "{{IgnoreBlockPublicAccess}}",
          "CloudWatchLogGroupName": "{{CloudWatchLogGroupName}}",
          "CloudWatchLogStreamName": "{{CloudWatchLogStreamName}}",
          "CreateLogandStreamError": "{{CreateLogandStream.error}}",
          "Verbose": "{{Verbose}}",
          "HttpGet": "{{HttpGet}}",
          "ExecutionId": "{{automation:EXECUTION_ID}}"
        },
        "Script": "# Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this\n# software and associated documentation files (the \"Software\"), to deal in the Software\n# without restriction, including without limitation the rights to use, copy, modify,\n# merge, publish, distribute, sublicense, and/or sell copies of the Software, and to\n# permit persons to whom the Software is furnished to do so.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,\n# INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A\n# PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT\n# HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\n# OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n# SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport sys, boto3, json, time, warnings, http.client, textwrap\nfrom collections import defaultdict\nfrom botocore.errorfactory import ClientError\n\nsys.tracebacklimit=0\nclient = boto3.client('s3')\nlogs = boto3.client('logs')\n\n# Object tags information when the bucket policy contains any s3:ExistingObjectTag condition key\ndef _get_object_tags(bucket,key_name):\n    try:\n        return client.get_object_tagging(Bucket=bucket,Key=key_name).get('TagSet',[])\n    except ClientError as ex:\n        _print_error(ex,key_name,'get object tags')\n        return []\n\ndef _simulate_custom_policy(bucket,policy,arn,verbose=False):\n    # https://github.com/boto/boto3/issues/2084\n    # Policy search based on tags is not supported\n    \"\"\"\n    Description:\n        Simulates the bucket policy for 's3:GetObject' (read) action\n    \n    Permission Required:\n        - 's3:SimulateCustomPolicy'\n    \n    Arguments:\n        bucket (str): Name of the S3 bucket\n        policy (str): Policy associated with the S3 bucket\n        arn: Arn of the object. Ex: 'arn:aws:s3:::{bucket}/{key_name}'\n    \n    Returns:\n        decision (str): EvaluationResults.EvalDecision (allowed | explicitDeny | implicitDeny | error)\n        missingContextValues (str): EvaluationResults.MissingContextValues A list of context keys that are \n            required by the included input policies but that were not provided by one of the input parameters\n    \"\"\"\n    \n    try:\n        response = boto3.client('iam').simulate_custom_policy(\n            PolicyInputList=[policy],\n            ResourceArns=[arn],\n            ActionNames=['s3:GetObject'])\n\n        results = response.get('EvaluationResults',[])\n        decision = response['EvaluationResults'][0].get('EvalDecision','')\n        missingContextValues = response['EvaluationResults'][0].get('MissingContextValues',[])\n        \n        if len(results) == 1:\n            return decision,missingContextValues\n        elif len(results) != 1:\n            _print_error('simulate_custom_policy returned multiple values, expected only 1',arn,'simulate policy')\n            return 'error',['error']\n    except ClientError as ex:\n        _print_error(ex,arn,'simulate policy')\n        return 'error',['error']\n    else:\n        return 'error',['error']\n\ndef _scan_public_urls(bucket,bucket_location,key,request_payer=False):\n    \"\"\"\n    Description:\n        Performs an HTTPS Get using the Range bytes=0-0 header to receive only the first byte.\n        When using \"Range\" it is expected to receive HTTP status 206 (Partial Content)\n    \n    Permission Required:\n        - N/A\n    \n    Arguments:\n        bucket (string): Name of the S3 bucket\n        bucket_location (string): ocation of the bucket (region)\n        key (string): object key name\n        request_payer (bool): if True, sets the x-amz-request-payer:requester header\n    \n    Returns:\n        response.status. Ex: 200, 206, 400, etc.\n        response.reason. Ex: 206 Partial Content, 200 OK, 400 Bad Request, etc.\n        req_id: 'x-amz-id-2' request header\n        host_id: 'x-amz-request-id' request header\n    \"\"\"\n    \n    try:\n        url=f'{bucket}.s3.{bucket_location}.amazonaws.com'\n        connection = http.client.HTTPSConnection(url,timeout=5)\n        if request_payer:\n            connection.request('GET', f'/{key}',headers={\"Range\":\"bytes=0-0\",\"x-amz-request-payer\":\"requester\"})\n        else:    \n            connection.request('GET', f'/{key}',headers={\"Range\": \"bytes=0-0\"})\n        response = connection.getresponse()\n        req_id = response.getheader('x-amz-id-2', default=None)\n        host_id = response.getheader('x-amz-request-id', default=None)\n        connection.close()\n        return response.status, response.reason, req_id, host_id\n    except Exception as e:\n        _print_error(e,key,'http get')\n        return 0,0,'error','error'\n\ndef _get_bucket_policy(bucket):\n    try:\n        return client.get_bucket_policy(Bucket=bucket).get('Policy')\n    except ClientError as ex:\n        if ex.response['Error']['Code'] == 'NoSuchBucketPolicy':\n            return {}\n        else:\n            _print_error(ex,bucket,'get bucket policy')\n            raise\n    \ndef _check_public_acl(grants,bucket,key_name=None):\n    PUBLIC_GROUP = { 'http://acs.amazonaws.com/groups/global/AllUsers': 'All Users' }\n    AUTHENTICATED_GROUP = { 'http://acs.amazonaws.com/groups/global/AuthenticatedUsers': 'Authenticated Users' }\n    # Anonymous canonical user ID https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html\n    ANONYMOUS_USER = '65a011a29cdf8ec533ec3d1ccaae921c'\n    \n    permissions = set()\n\n    for grant in grants:\n        uri = grant['Grantee'].get('URI')\n        uid = grant['Grantee'].get('ID')\n        name = bucket\n        resource = 'bucket'\n        if key_name is not None:\n            name = key_name\n            resource = 'object'\n        \n        if uri in PUBLIC_GROUP.keys() and grant['Permission'] in ('READ','FULL_CONTROL'):\n            permissions.add(PUBLIC_GROUP[uri])\n        if uid == ANONYMOUS_USER and grant['Permission'] in ('READ','FULL_CONTROL'):\n            permissions.add(ANONYMOUS_USER)\n    \n    return permissions\n\ndef _get_bucket_payer(bucket):\n    try:\n        response = client.get_bucket_request_payment(Bucket=bucket)\n        return response.get('Payer')\n    except ClientError as ex:\n        _print_error(ex,key,'get bucket payer')\n        return 'error'\n\ndef _get_object_acl(bucket,key,caller_s3_user):\n    try:\n        response = client.get_object_acl(Bucket=bucket,Key=key)\n        owner_id = response['Owner']['ID']\n        return response['Grants'], False\n    except ClientError as ex:\n        _print_error(ex,key,'get object acl')\n        return [], True\n    \ndef _check_encryption(bucket,key_name):\n    try:\n        response = client.head_object(Bucket=bucket,Key=key_name)\n        return response.get('ServerSideEncryption')\n    except ClientError as ex:\n        _print_error(ex,key_name,'encryption')\n        return 'error'\n\ndef _print_error(err,name,module_name='',err_type='error'):\n    try:\n        if type(err) == ClientError:\n            print(f'[{err_type}] [{module_name}] [{name}] code:{err.response[\"Error\"][\"Code\"]} message:{err.response[\"Error\"][\"Message\"]}')\n        else:\n            print(f'[{err_type}] [{module_name}] [{name}] {str(err)}')\n    except Exception as e:\n        print(f'Error: {str(e)}')\n        \ndef _get_object_metadata(bucket,key_name):\n    try:\n        response = client.head_object(Bucket=bucket,Key=key_name)\n        return response\n    except ClientError as ex:\n        _print_error(ex,key_name,'head object')\n        return {}\n    \ndef _list_objects(bucket, prefix, start_after, limit):\n    try:\n        if limit > 100:\n            raise ValueError(\"List objects MaxItems greater than 100 is not supported\")\n        paginator = client.get_paginator(\"list_objects_v2\")\n        page_iterator = paginator.paginate(Bucket=bucket, Prefix=prefix, StartAfter=start_after,\n            FetchOwner=True, PaginationConfig={'MaxItems': limit})\n        keys = defaultdict(str)\n        for page in page_iterator:\n            if 'Contents' in page:\n                for key in page['Contents']:\n                    keys[key['Key']]=key['Owner']['ID']\n        return keys\n    except ClientError as ex:\n        _print_error(ex,bucket,'list objects')\n        if ex.response['Error']['Code'] == 'NoSuchBucket':\n            return keys\n        else:\n            raise\n\n# Handles the semantics of uploading batches to CloudWatch Logs\nclass CWLogger(object):\n    # 1MB Max Size of a batch of log events (Service Limit)\n    MAX_BATCH_SIZE_BYTES = 1000000\n    # 10K max events in a batch (Service Limit)\n    MAX_BATCH_COUNT = 9999\n    # Each log event has a 26 byte overhead\n    LOGEVENT_OVERHEAD_BYTES = 26\n    \n    ERROR_CODES = {\n        'I00':'Starting S3 bucket analysis.',\n        'W01':'Automation document IgnoreBlockPublicAccess input parameter is set to true.',\n        'E01':'S3 bucket or account block public access settings are configured to ignore public ACLs and any public bucket policy.',\n        'I01':'Starting object analysis.',\n        'W02':'Object name ending with a single period (.) or two periods (..), prevents you downloading the object using the Amazon S3 console.',\n        'W03':'Object owner is canonical anonymous user {}.',\n        'W04':'Object owner user id {}.. does not match the account user id {}..',\n        'I02':'Object owner user id {}.. match the account user id {}..',\n        'E02':'Block block public access settings are configured to ignore public ACLs and bucket policy is not used because the object owner is not the bucket owner.',\n        'I03':'Object ACL does not grant public permissions.',\n        'I04':'Object ACL grants read permissions for {}.',\n        'E03':'Object ACLs is not public and bucket policy is not used for granting access because the object owner is not the bucket owner. Note: Denying access it is still used to deny access to objects the bucket owner does not own.',\n        'E04':'Object ACL does not grant public permissions and the block public access settings are configured to ignore public policies.',\n        'I05':'Simulate bucket policy allows s3:GetObject permissions for object.',\n        'E05':'Simulate bucket policy found an s3:GetObject explicit deny that match object arn.',\n        'W05':'Simulate bucket policy found an s3:GetObject implicit deny that match object arn.',\n        'W09':'Simulate bucket policy found an s3:GetObject implicit deny for the object arn with MissingContextValues {}. Check the bucket policy Conditions section',\n        'E06':'Neither the object ACL or bucket policy grant public read permissions for object.',\n        'E07':'Bucket policy does not grant public permissions for object and block public access settings are configured to ignore public ACLs.',\n        'I06':'Object has website redirection configured.',\n        'W06':'Object version deleted (delete marker). For more information: https://docs.aws.amazon.com/AmazonS3/latest/dev/DeleteMarker.html.',\n        'W07':'Object legal hold status is ON.',\n        'E08':'Object storage class {} not supported for public read access.',\n        'E09':'Object is encrypted using KMS.',\n        'I07':'Object is not encrypted using KMS.',\n        'I08':'No public access configuration issues detected. Please check the [error] or [warn] entries.',\n        'W08':'Object analysis did not detect public access configuration issues, however the IgnoreBlockPublicAccess document input is enabled (True).',\n        'I09':'HTTP GET request status:{}, reason:{}.',\n        'E10':'HTTP GET request status:{}, reason:{}, x-amz-request-id:{}, x-amz-id-2:{}.',\n        'I10':'Object analysis finished.',\n        'E11':'list_objects operations did not find any object with prefix:{}.',\n        'E12':'S3 bucket analysis detected some public access configuration issues. Please review the AnalyzeObjects output step for more details.',\n        'I11':'S3 bucket analysis did not detect public access configuration issues. Please review the AnalyzeObjects output step for more details.',\n        'I12':'S3 bucket analysis finished.',\n        'E13':'Unhandled error {}.',\n        'W10':'Object has the following tags {}.',\n        'E14':'Unhandled policy simulation error result EvalDecision: {}, MissingContextValues: {}.',\n        'E15':'Error getting object ACL.',\n        'E16':'Error getting object metadata.',\n        'E17':'Error getting object server side encryption configuration.',\n        'E18':'Object ACL does not grant public permissions and bucket has no policy associated.',\n        'E19':'Object analysis detected some public access configuration issues. Please review the previous output for details.',\n        'I13':'Object metadata replication status: {}.',\n        'W11':'Account block public access settings are configured to ignore public ACLs.',\n        'W12':'Account block public access settings are configured to ignore any public bucket policy.',\n        'W13':'S3 bucket block public access settings are configured to ignore public ACLs.',\n        'W14':'S3 bucket block public access settings are configured to ignore any public bucket policy.',\n\n    }\n    \n    def __init__(self, log_group, log_stream, bucket, execution_id, max_retries_on_put=5, enabled=False, debug=False):\n        # The Log Group and Log Stream\n        self.log_group = log_group\n        self.log_stream = log_stream\n        # Max number of times to retry a failed PutLogEvents request for a batch of events\n        self.max_retries_on_put = max_retries_on_put\n        # Feature enabled\n        self.enabled = enabled\n        self.debug = debug\n        # Current sequence token\n        self.sequence_token = None\n        # Current batch being buffered\n        self.current_batch_size = 0\n        self.current_batch = []\n\n        self.output = defaultdict(list)\n        self.bucket = bucket\n        self.execution_id = execution_id\n    \n    def report(self):\n        dic = self.output.copy()\n        summary = self.prettify({self.bucket:dic.pop(self.bucket,{})})\n        report = self.prettify(dic)\n        return json.dumps(self.output), summary, report       \n    \n    def prettify(self,dic,indent=20,width=160):\n        txt = ''\n        for k,v in dic.items():\n            txt += f'{k}\\n'\n            for r in v:\n                wrapped = textwrap.fill(r['Description'],\n                            initial_indent='',\n                            subsequent_indent=' ' * indent,\n                            width=width,\n                            )\n                txt +='[{:<5}] [{:<3}] {}\\n'.format(r['Status'],r['Code'],r['Description'])\n            txt += '\\n'\n        \n        return txt\n      \n    def push_event(self,name,code,*args):\n        \"\"\"\n        @type level: int\n        @param level: 0='info' 1='warn', 2='error', 3='debug'\n        \"\"\"\n        level = 'debug'\n        if code.startswith('I'):\n            level = 'info'\n        elif code.startswith('W'):\n            level = 'warn'\n        elif code.startswith('E'):\n            level = 'error'           \n            \n        timestamp = int(round(time.time() * 1000))\n\n        msg = self.ERROR_CODES[code].format(*args)\n        log_event = {\n            \"timestamp\" : timestamp,\n            \"message\": f'[{self.execution_id}] [{level}] [{name}] [{code}] {msg}'\n        }\n            \n        event_size = len(log_event['message'].encode('utf-8')) + self.LOGEVENT_OVERHEAD_BYTES\n        if len(self.current_batch) == self.MAX_BATCH_COUNT or event_size + self.current_batch_size  >= self.MAX_BATCH_SIZE_BYTES:\n            self.flush()\n\n        self.current_batch.append(log_event)\n        self.current_batch_size += event_size\n         \n        if self.debug or (level in ('warn','error','debug')) or code == 'I09':\n            self.output[name].append({ 'Status':level,'Code':code,'Description':self.ERROR_CODES[code].format(*args) })\n    \n    def flush(self):\n        try:\n            if len(self.current_batch) > 0 and self.enabled and self.log_group and self.log_stream:\n                _print_error(f'Pushing batch with {len(self.current_batch)} events to CloudWatch','logs','flush','info')\n                self.send_batch(self.current_batch)\n                self.current_batch = []\n                self.current_batch_size = 0\n        except Exception as e:\n            _print_error(e,'logs','flush')\n            \n    def send_batch(self, batch):\n        if len(batch) > 0 and self.enabled:\n            batch.sort(key=lambda event: event['timestamp'])\n            if self.enabled:\n                try:\n                    self.put_log_events_with_retry(batch)\n                except Exception as e:\n                    _print_error(e,'logs','send batch')\n\n    def put_log_events_with_retry(self, batch):\n        retry_num = 0\n        while retry_num < self.max_retries_on_put:\n            retry_num += 1\n            try:\n                if self.debug and retry_num > 2:\n                    _print_error(f'retry: {retry_num}','cloudwatch logs','put log event','debug')\n                if self.sequence_token:\n                    response = logs.put_log_events(logGroupName=self.log_group, logStreamName=self.log_stream, logEvents=batch, sequenceToken=self.sequence_token)\n                else:\n                    response = logs.put_log_events(logGroupName=self.log_group, logStreamName=self.log_stream, logEvents=batch)\n\n                self.sequence_token = response[\"nextSequenceToken\"]\n                return self.sequence_token\n            except Exception as e:\n                if \"ResourceNotFoundException\" in str(e):\n                    pass\n                elif \"DataAlreadyAcceptedException\" in str(e):\n                    # Skipping as this payload has already been accepted.\n                    self.sequence_token = self.parse_next_sequence_token_from_exception(e)\n                    return self.sequence_token\n                elif \"InvalidSequenceTokenException\" in str(e):\n                    # Re-trying with next sequence token.'\n                    self.sequence_token = self.parse_next_sequence_token_from_exception(e)\n                else:\n                    # Sleep before the retry to avoid throttling\n                    time.sleep(0.200)\n        \n        _print_error(f'failed to put batch after {retry_num} retries','logs','flush')\n        raise Exception(\"Unable to send batch.\")\n\n    def parse_next_sequence_token_from_exception(self, e):\n        parsed_token = None\n        if \"sequenceToken: \" in str(e):\n            # Format for DataAlreadyAcceptedExceptions\n            parsed_token = str(e).split(\"sequenceToken: \")[1];\n        elif \"sequenceToken is: \" in str(e):\n            # Format for InvalidSequenceTokenException\n            parsed_token = str(e).split(\"sequenceToken is: \")[1];\n        else:\n            _print_error('could not parse the sequence token from the exception:'+str(e),'logs','parse next token')\n            parsed_token = None\n\n        if parsed_token == \"null\":\n            # This happens when sending log events with a token to\n            # a stream that doesn't expect a token.\n            parsed_token = None\n\n        return parsed_token\n\ndef _get_public_access_settings(account_id,bucket_name=None):\n    \"\"\"\n    Description:\n        Calls s3control.GetPublicAccessBlock and s3.GetPublicAccessBlock to retrieves the \n        PublicAccessBlock configuration for the Amazon S3 bucket and the AWS Account.\n        Reference: https://docs.aws.amazon.com/AmazonS3/latest/dev/access-control-block-public-access.html\n    \n    Permission Required:\n        - 's3:GetBucketPublicAccessBlock'\n    \n    Arguments:\n        bucket_name (str): Name of the S3 bucket\n        account_id (str): AWS Account Id\n    \"\"\"\n        \n    try:\n        if bucket_name is None:\n            settings = boto3.client('s3control').get_public_access_block(AccountId=account_id)\n        else:\n            settings = client.get_public_access_block(Bucket=bucket_name)\n        \n        return settings['PublicAccessBlockConfiguration']\n    \n    except ClientError as ex:\n        if ex.response['Error']['Code'] == 'NoSuchPublicAccessBlockConfiguration':\n            return {\n                'BlockPublicAcls': False,\n                'IgnorePublicAcls': False,\n                'BlockPublicPolicy': False,\n                'RestrictPublicBuckets': False\n            }\n        else:\n            print(f'Error: {ex.response}')\n            raise\n        \ndef analyze_objects(events,context):\n    \"\"\"\n    Description:\n        List the objects from the bucket using s3.list_objects_v2 given the Automation \n        document parameters: S3BucketName, S3PrefixName, StartAfter, and MaxObjects in order\n        to check if the bucket or object configuration that prevents the object to be public readable\n\n    Permission Required:\n        - 'logs:PutLogEvents'\n        - 'iam:SimulateCustomPolicy'\n        - 'iam:GetContextKeysForCustomPolicy'\n        - 's3:GetObject'\n        - 's3:GetObjectAcl'\n        - 's3:ListBucket'\n        - 's3:GetObject'\n        - 's3:GetObjectAcl'\n        - 's3:GetObjectTagging'\n        \n    Arguments:\n        events.S3BucketName (string): Name of the S3 bucket\n        events.S3PrefixName (string): Shared name prefix for objects or object name\n        events.StartAfter (string): Starts listing after this specified key\n        events.MaxObjects (integer): Max number of objects to be returned\n        events.GetBucketInformationLocation (string): S3 bucket location\n        events.GetBucketInformationOwner (string): S3 owner canonical user\n        events.GetBlockPublicAccessIgnoreACL (bool): IgnorePublicAcls setting ON/OFF\n        events.GetBlockPublicAccessIgnorePolicy (bool): RestrictPublicBuckets setting ON/OFF\n        events.IgnoreBlockPublicAccess (bool): IgnoreBlockPublicAccess automation input parameter. Ignores Public Access Settings\n        events.Verbose (bool): Return INFO and DEBUG information during the analysis\n        events.CloudWatchLogGroupName (string): S3 bucket location\n        events.CloudWatchLogStreamName (string): S3 owner canonical user\n        events.CreateLogandStreamError (bool): True if CW LogGroup and Stream was created succesfully, False otherwise\n        events.HttpGet (bool): True if you want to perform HTTP.GET\n        events.ExecutionId (string): Automation Execution Id used for pushing logs to CW\n        \n    Returns:\n        error (bool): True if any issue is found in the configuration, False otherwise\n        json: analysis output in JSON format (only for step output)\n        summary: bucket analysis section for Automation overall output\n        report: object analysis section for Automation overall output\n    \"\"\"\n    account_id = events['AccountId']\n    bucket = events['S3BucketName']\n    prefix = events.get('S3PrefixName','')\n    start_after = events.get('StartAfter','')\n    limit = int(events.get('MaxObjects',5))\n    bucket_location = events['GetBucketInformationLocation']\n    bucket_owner = events['GetBucketInformationOwner']\n    ignore_block_public = events['IgnoreBlockPublicAccess']\n    verbose = events.get('Verbose',False)\n    log_group = events.get('CloudWatchLogGroupName')\n    log_stream = events.get('CloudWatchLogStreamName')\n    enabled = not events.get('CreateLogandStreamError',False)\n    http_get = events.get('HttpGet',False)\n    execution_id = events.get('ExecutionId','id')\n    \n    logger = CWLogger(log_group, log_stream, bucket, execution_id, 5, enabled, verbose)\n    logger.push_event(bucket,'I00')\n    \n    block_ignore_public_acl = False\n    block_ignore_public_policy = False\n                  \n    if ignore_block_public:\n        logger.push_event(bucket,'W01')\n        # The analysis ignores bucket and account public access settings\n    else:\n        account_settings = _get_public_access_settings(account_id)\n        \n        if account_settings.get('IgnorePublicAcls'):\n            logger.push_event(account_id,'W11')\n            block_ignore_public_acl = True\n        if account_settings.get('RestrictPublicBuckets'):\n            logger.push_event(account_id,'W12')\n            block_ignore_public_policy = True\n\n        settings = _get_public_access_settings(account_id,bucket)\n        \n        if settings.get('IgnorePublicAcls'):\n            logger.push_event(bucket,'W13')\n            block_ignore_public_acl = True\n        if settings.get('RestrictPublicBuckets'):\n            logger.push_event(bucket,'W14')\n            block_ignore_public_policy = True\n\n    if block_ignore_public_acl and block_ignore_public_policy:\n        logger.push_event(bucket,'E01')\n        dump, summary, report = logger.report()\n        return { 'error':True, \n            'json': dump,\n            'summary':summary,\n            'report':report\n            }  \n    \n    bucket_policy = _get_bucket_policy(bucket)\n    obj_num = 0\n    issue_found = False\n    try:\n        for key_name, object_owner in _list_objects(bucket, prefix, start_after, limit).items():\n            if key_name.endswith('/'):\n                continue  # cut early, next object\n            obj_num += 1\n            \n            if block_ignore_public_acl and block_ignore_public_policy:\n                logger.push_event(key_name,'E01')\n                issue_found = True\n                continue  # cut early, next object\n            \n            if http_get:\n                get_status,get_reason,req_id,host_id = _scan_public_urls(bucket,bucket_location,key_name)\n                if get_status in (200,206):\n                    logger.push_event(key_name,'I09',get_status,get_reason)\n                    continue  # cut early, next object\n                else:\n                    logger.push_event(key_name,'E10',get_status,get_reason,req_id,host_id)\n                    issue_found = True\n                \n            # https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html\n            if key_name.endswith('.') or key_name.endswith('..'):\n                logger.push_event(key_name,'W02')\n            # If OBJECT owner does not match caller S3 canonical user or owner is anonymus (ANONYMOUS_USER) print a Warning\n            ANONYMOUS_USER = '65a011a29cdf8ec533ec3d1ccaae921c'\n            if object_owner != bucket_owner:\n                if object_owner == ANONYMOUS_USER:\n                    logger.push_event(key_name,'W03',ANONYMOUS_USER)\n                else:\n                    logger.push_event(key_name,'W04',object_owner[:10],bucket_owner[:10])\n            else:\n                # Bucket owner and object owner match\n                logger.push_event(key_name,'I02',object_owner[:10],bucket_owner[:10])\n            \n            if (object_owner != bucket_owner) and block_ignore_public_acl:\n                logger.push_event(key_name,'E02')\n                issue_found = True\n                continue  # cut early, next object\n            \n            object_acl, error_acl = _get_object_acl(bucket,key_name,bucket_owner)\n            object_public_acl = _check_public_acl(object_acl,bucket,key_name)\n            if error_acl:\n                logger.push_event(key_name,'E15')\n                issue_found = True\n            else:\n                if len(object_public_acl) == 0:\n                    logger.push_event(key_name,'I03')\n                else:\n                    for group in object_public_acl:\n                        logger.push_event(key_name,'I04',group)\n            \n                if len(object_public_acl) == 0 and (object_owner != bucket_owner):\n                    logger.push_event(key_name,'E03')\n                    continue  # cut early, next object\n                \n                if len(object_public_acl) == 0 and block_ignore_public_policy:\n                    logger.push_event(key_name,'E04')\n                    issue_found = True\n                    continue  # cut early, next object\n           \n            # TODO: s3:ExistingObjectTag/<key>\tRequires that an existing object tag has a specific tag key and value.\n            tags = _get_object_tags(bucket,key_name)\n            # Log object tags for s3:ExistingObjectTag condition analysis\n            if len(tags) > 0:\n                logger.push_event(key_name,'W10',tags)\n            \n            if bucket_policy:\n                partition = 'aws'\n                if bucket_location.startswith('cn-'):\n                    partition = 'aws-cn'\n                if bucket_location.startswith('us-gov'):\n                    partition = 'aws-us-gov'\n                \n                arn = f'arn:{partition}:s3:::{bucket}/{key_name}'\n                evaluation, missingContextValues = _simulate_custom_policy(bucket,bucket_policy,arn,verbose)\n                if evaluation == 'allowed':\n                    logger.push_event(key_name,'I05')\n                elif evaluation == 'explicitDeny':\n                    logger.push_event(key_name,'E05')\n                    issue_found = True\n                    continue  # cut early, next object\n                elif evaluation == 'implicitDeny':\n                    # There are conditions in the bucket policy that require analysis\n                    if len(missingContextValues) > 0:\n                        logger.push_event(key_name,'W09',missingContextValues)\n                        issue_found = True\n                    else:\n                        logger.push_event(key_name,'W05')\n                        if len(object_public_acl) == 0:\n                            logger.push_event(key_name,'E06')\n                            issue_found = True\n                            continue  # cut early, next object\n                        if block_ignore_public_acl:\n                            logger.push_event(key_name,'E07')\n                            issue_found = True\n                            continue  # cut early, next object\n                else: # evaluation == 'error'\n                    logger.push_event(key_name,'E14',evaluation,missingContextValues)    \n            else: # No bucket policy attached\n                if len(object_public_acl) == 0:\n                    logger.push_event(key_name,'E18')\n            \n            \n            object_metadata = _get_object_metadata(bucket,key_name)\n            if not object_metadata:\n                logger.push_event(key_name,'E16')\n                issue_found = True\n            if object_metadata.get('WebsiteRedirectLocation'):\n                logger.push_event(key_name,'I06')\n            if object_metadata.get('DeleteMarker'):\n                logger.push_event(key_name,'W06')\n            if object_metadata.get('ReplicationStatus'):\n                logger.push_event(key_name,'I13', object_metadata.get('ReplicationStatus'))\n            if object_metadata.get('ObjectLockLegalHoldStatus') == 'ON':\n                logger.push_event(key_name,'W07')\n            if object_metadata.get('StorageClass'):\n                if object_metadata['StorageClass'] in ('GLACIER','DEEP_ARCHIVE'):\n                    logger.push_event(key_name,'E08',object_metadata['StorageClass'])\n                    issue_found = True\n                    continue  # cut early, next object\n                    \n            sse = _check_encryption(bucket,key_name)\n            if sse == 'aws:kms':\n                logger.push_event(key_name,'E09')\n                issue_found = True\n                continue  # cut early, next object\n            elif sse == 'error':\n                logger.push_event(key_name,'E17')\n                issue_found = True\n            else: # 'AES256'\n                logger.push_event(key_name,'I07')\n                \n            if issue_found:\n                logger.push_event(key_name,'E19')\n            else:\n                if ignore_block_public:\n                    logger.push_event(key_name,'W08')\n                else:\n                    logger.push_event(key_name,'I08')\n                \n            continue # next object\n\n        if obj_num == 0:\n            logger.push_event(bucket,'E11',prefix)\n            dump, summary, report = logger.report()\n            return { 'error':True, \n                'json':dump,\n                'summary':summary,\n                'report':report\n                }              \n        else:\n            if issue_found:\n                logger.push_event(bucket,'E12')\n            else:\n                logger.push_event(bucket,'I11')\n\n        logger.push_event(bucket,'I12')\n        dump, summary, report = logger.report()\n        return { 'error':issue_found, \n                'json': dump,\n                'summary': summary,\n                'report':report\n                }\n    \n    except Exception as e:\n        _print_error(e,'object analysis','analysis')\n        logger.push_event(bucket,'E13',str(e))\n        raise\n    \n    finally:\n        #Push events to CloudWatch\n        logger.flush()\n"
      },
      "outputs": [
        {
          "Name": "json",
          "Selector": "$.Payload.json",
          "Type": "String"
        },
        {
          "Name": "error",
          "Selector": "$.Payload.error",
          "Type": "Boolean"
        },
        {
          "Name": "bucket",
          "Selector": "$.Payload.summary",
          "Type": "String"
        },
        {
          "Name": "objects",
          "Selector": "$.Payload.report",
          "Type": "String"
        }
      ]
    }
  ],
  "outputs": [
    "AnalyzeObjects.objects",
    "AnalyzeObjects.bucket"
  ]
}
