{
  "schemaVersion": "0.3",
  "description": "# AWSSupport-TroubleshootEKSWorkerNode\n\n----\n\nThe *AWSSupport-TroubleshootEKSWorkerNode* runbook is designed to help troubleshooting EKS worker node that [failed to join an EKS cluster](https://docs.aws.amazon.com/eks/latest/userguide/troubleshooting.html#worker-node-fail). This automation runbook checks both EKS cluster configuration and worker node and validates the following:\n\n- Node tags are applied.\n- Worker node Instance type is supported. \n- Network communication between worker node and Cluster API server is allowed. \n- Node [IAM Role and Policies](https://docs.aws.amazon.com/eks/latest/userguide/create-node-role.html).\n- Cluster [IAM role and Policies](https://docs.aws.amazon.com/eks/latest/userguide/service_IAM_role.html).\n- VPC Endpoints for [private Clusters](https://docs.aws.amazon.com/eks/latest/userguide/private-clusters.html). \n- Worker node [AMI version](https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html). \n- VPC DHCP optionset. \n- Kubelet, container runtime.\n\n\n### Prerequisites:\n\n* To check Kubelet status and container runtime, EC2 instance must be an online managed instance(connected with AWS Systems Manager). If your EC2 instance is not an online managed instance, only this step of the test will be skipped, however all other checks will be executed.\n\n### Disclaimer:\n\n* This runbook doesn't make any changes to your EKS cluster or your worker node.\n* This runbook doesn't support worker nodes running Windows or Bottlerocket Operating Systems.\n\n### Workflow Specifications:\n\n This workflow uses AWS Systems Manager Automation and takes in the following parameters:\n\n- **ClusterName** - **(Required)**: The EKS cluster name. \n- **WorkerID** - **(Required)** : Worker node that failed to join the EKS cluster.\n- **AutomationAssumeRole** - **(Optional)** The IAM role which AWS Systems Manager will assume to execute this automation. This role must allow these IAM actions:\n\n        - iam:GetRole\n        - iam:GetInstanceProfile\n        - iam:ListAttachedRolePolicies\n        - ec2:DescribeDhcpOptions\n        - ec2:DescribeNatGateways\n        - ec2:DescribeSecurityGroups\n        - ec2:DescribeImages\n        - ec2:DescribeNetworkInterfaces\n        - ec2:DescribeVpcs\n        - ec2:DescribeVpcEndpoints\n        - ec2:DescribeSubnets\n        - ec2:DescribeNetworkAcls\n        - ec2:DescribeInstanceStatus\n        - ec2:DescribeInstances\n        - ec2:DescribeRouteTables\n        - ec2:DescribeVpcAttribute\n        - ec2:DescribeInstanceAttribute\n        - eks:DescribeCluster\n        - ssm:DescribeInstanceInformation\n        - ssm:ListCommandInvocations\n        - ssm:ListCommands\n        - ssm:SendCommand\n\n\n\nPlease visit the documentation on [Automation Setup](https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-setup.html) for more information.",
  "assumeRole": "{{ AutomationAssumeRole }}",
  "parameters": {
    "AutomationAssumeRole": {
      "type": "AWS::IAM::Role::Arn",
      "description": "(Optional) The ARN of the role that allows the Automation runbook to perform the actions on your behalf. If no role is specified, Systems Manager Automation uses your current IAM user permissions context to execute this runbook.",
      "default": ""
    },
    "ClusterName": {
      "type": "String",
      "description": "(Required) The name of the EKS cluster",
      "allowedPattern": "^[a-zA-Z0-9][a-zA-Z0-9-_]{0,99}$"
    },
    "WorkerID": {
      "type": "String",
      "description": "(Required) The EC2 instance ID for the Worker node which failed to join the cluster",
      "default": "",
      "allowedPattern": "^i-[a-z0-9]{8,17}$"
    }
  },
  "mainSteps": [
    {
      "name": "validateIfClusterExistsAndActive",
      "action": "aws:assertAwsResourceProperty",
      "isCritical": true,
      "timeoutSeconds": 30,
      "maxAttempts": 3,
      "inputs": {
        "Service": "eks",
        "Api": "DescribeCluster",
        "name": "{{ClusterName}}",
        "PropertySelector": "$.cluster.status",
        "DesiredValues": [
          "ACTIVE"
        ]
      }
    },
    {
      "name": "validateIfInstanceExistsAndRunning",
      "action": "aws:assertAwsResourceProperty",
      "isCritical": true,
      "timeoutSeconds": 30,
      "maxAttempts": 3,
      "inputs": {
        "Service": "ec2",
        "Api": "DescribeInstanceStatus",
        "InstanceIds": [
          "{{WorkerID}}"
        ],
        "PropertySelector": "$.InstanceStatuses[0].InstanceState.Name",
        "DesiredValues": [
          "running"
        ]
      }
    },
    {
      "name": "TroubleshootWorkerNode",
      "action": "aws:executeScript",
      "isCritical": true,
      "inputs": {
        "Runtime": "python3.8",
        "InputPayload": {
          "ClusterName": "{{ClusterName}}",
          "WorkerID": "{{WorkerID}}"
        },
        "Handler": "script_handler",
        "Script": "# Copyright 2022 Amazon.com, Inc. and its affiliates. All Rights Reserved.\n# SPDX-License-Identifier: LicenseRef-.amazon.com.-AmznSL-1.0\n# Licensed under the Amazon Software License (the \"License\").\n# You may not use this file except in compliance with the License.\n# A copy of the License is located at\n#   http://aws.amazon.com/asl/\n# or in the \"license\" file accompanying this file. This file is distributed\n# on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n# express or implied. See the License for the specific language governing\n# permissions and limitations under the License.\n\nfrom base.node import Node\nfrom base.cluster import Cluster\nfrom base.messages import Messages\nfrom case import cluster_enis\nfrom case import ClusterSec_check\nfrom case import cp_iam_role\nfrom case import dhcp_options\nfrom case import iam_role_check\nfrom case import node_tags\nfrom case import node_ami\nfrom case import cluster_public_endpoint_access\nfrom case import vpc_dns\nfrom case import node_cni\nfrom case import instancetype\nfrom case import nacls_basic_rules\nfrom case import sts_endpoint\nfrom case import node_iam_role\nfrom case import public_subnet_IP\nfrom case import s3_endpoint_route\nfrom case import userdata\nfrom case import verifyNat\nfrom case import vpc_endpoint_check\nfrom case import node_sg_outbound\nfrom case import outposts_check\nfrom case import sw_daemons\nfrom case import imds_check\n\ndef script_handler(events, context):\n\n    worker_node_id = events['WorkerID']\n    cluster_name = events['ClusterName']\n\n    messages = Messages()\n\n    cluster = Cluster(cluster_name)\n    node = Node(worker_node_id)\n\n    if not cluster.isActive:\n        messages.render('I0',\n                        'ERROR',\n                        messages.generalMessages['clusterNotActive'],\n                        cluster_name=cluster_name)\n    else:\n        messages.render('I0',\n                        'NODASH',\n                        messages.generalMessages['clusterActive'],\n                        cluster_name=cluster_name)\n        if node.IsTerminated:\n            messages.render('I0',\n                            'ERROR',\n                            messages.nodeMessages['terminatedInstance'],\n                            node_id=worker_node_id)\n        else:\n            messages.render('I2',\n                            'NODASH',\n                            messages.clusterMessages['checkingClusterSG'])\n            if ClusterSec_check.ClusterSecuritygroup(node.privateIpAddress,\n                                                     node.securityGroupIDs,\n                                                     cluster.clusterSecurityGroup):\n                messages.render('I3',\n                                'INFO',\n                                messages.clusterMessages['clusterSGOK'], cluster_sg=cluster.clusterSecurityGroup)\n            else:\n                messages.render('I3',\n                                'ERROR',\n                                messages.clusterMessages['clusterSGNOK'], cluster_sg=cluster.clusterSecurityGroup)\n            messages.render('I2',\n                            'NODASH',\n                            messages.networkMessages['checkingDHCP'])\n            if dhcp_options.dhcp_options_check(node.InstanceVpc):\n                messages.render('I3',\n                                'INFO',\n                                messages.networkMessages['dhcpOK'])\n            else:\n                messages.render('I3',\n                                'ERROR',\n                                messages.networkMessages['dhcpNOK'])\n\n            messages.render('I2',\n                            'NODASH',\n                            messages.clusterMessages['checkingServiceRole'],\n                            cluster_role=cluster.roleArn)\n            cp_iam_role.check_cluster_role(cluster.roleArn,\n                                           cluster_name,\n                                           messages)\n                \n            messages.render('I2',\n                            'NODASH',\n                            messages.networkMessages['checkingClusterENIs'])\n            cluster_enis.execute_checks(cluster, messages)\n            if cluster.isOnlyPublic:\n                cluster_public_endpoint_access.execute_checks(node, cluster, messages)\n            \n            messages.render('I2',\n                            'NODASH',\n                            messages.networkMessages['checkingVpcDNS'])\n            vpc_dns.check_vpc_attributes(cluster.vpcId, messages)\n\n##############worker node checks#############\n            messages.render('I0',\n                            'NODASH',\n                            messages.generalMessages['thinSeperator'])\n            messages.render('I0',\n                            'NODASH',\n                            messages.nodeMessages['instanceStateOK'],\n                            node_id=worker_node_id)\n            messages.render('I2',\n                            'NODASH',\n                            messages.nodeMessages['checkingInstanceFamily'])\n            if instancetype.validateInstanceType(node.InstanceType):\n                messages.render('I3',\n                                'INFO',\n                                messages.nodeMessages['instanceFamilyOK'],\n                                instance_family=node.InstanceType)\n            else:\n                messages.render('I3',\n                                'ERROR',\n                                messages.nodeMessages['instanceFamilyNOK'],\n                                instance_family=node.InstanceType)\n            messages.render('I2',\n                            'NODASH',\n                            messages.nodeMessages['checkingInstanceNetworking'])\n            if node.IsPrivate:\n                messages.render('I3',\n                                'INFO',\n                                messages.nodeMessages['privateInstance'],\n                                nat_gateway=node.NatGatway)\n                isNatPublic = verifyNat.verify_nat(node.NatGatway, node.InstanceVpc)\n                if not isNatPublic:\n                    messages.render('I4',\n                                    'ERROR',\n                                    messages.nodeMessages['privateNAT'],\n                                    nat_gateway=node.NatGatway)\n                else:\n                    messages.render('I4',\n                                    'INFO2',\n                                    messages.nodeMessages['natOK'],\n                                    nat_gateway=node.NatGatway)\n            elif node.IsPublic:\n                messages.render('I3',\n                                'INFO',\n                                messages.nodeMessages['publicInstance'],\n                                internet_gateway=node.InternetGateway)\n                if node.PublicIpAddress:\n                    messages.render('I4',\n                                    'INFO2',\n                                    messages.nodeMessages['instanceWithPublicIP'],\n                                    public_ip=node.PublicIpAddress)\n                else:\n                   messages.render('I4',\n                                    'ERROR',\n                                    messages.nodeMessages['instanceWithoutPublicIP'])\n\n                if public_subnet_IP.is_public_ip_map(node.InstanceSubnet):\n                    messages.render('I4',\n                                    'INFO2',\n                                    messages.nodeMessages['instanceAutoAssignIP'],\n                                    instance_subnet=node.instance_subnet_id)\n                else:\n                    messages.render('I4',\n                                    'ERROR',\n                                    messages.nodeMessages['instanceNoAutoAssingIP'],\n                                    instance_subnet=node.instance_subnet_id)\n            else:\n                messages.render('I3',\n                                'INFO',\n                                messages.nodeMessages['privateInstanceVPCEndpoint'])\n                messages.render('I3',\n                                'INFO',\n                                messages.networkMessages['checkingVPCE'])\n                vpc_endpoint_check.vpce_all_checks(node.Region,\n                                                   node.InstanceVpc,\n                                                   node.InstanceSubnet,\n                                                   node.privateIpAddress,\n                                                   node.securityGroupIDs,\n                                                   messages)\n                s3_endpoint_route.check_s3_endpoints(node, messages)\n            \n            messages.render('I2',\n                            'NODASH',\n                            messages.nodeMessages['checkingInstanceProfile'])\n            if node.Instanceprofile:\n                messages.render('I3',\n                                'INFO',\n                                messages.nodeMessages['displayInstanceProfile'],\n                                node_id=node.name,\n                                instance_profile=node.Instanceprofile)\n                if iam_role_check.iam_role_check(node.IAMRole):\n                    messages.render('I4',\n                                    'INFO2',\n                                    messages.nodeMessages['instanceProfileOK'],\n                                    instance_role=node.IAMRole)\n                else:\n                    messages.render('I4',\n                                    'ERROR',\n                                    messages.nodeMessages['instanceProfilePath'],\n                                    instance_role= node.IAMRole)\n                checkTrustRelationship = node_iam_role.check_trust_relationship(\n                    node.role_name\n                )\n                checkIAMPolicy = node_iam_role.check_instance_role(node.role_name)\n                if not checkIAMPolicy[0]:\n                    messages.render('I4',\n                                    'ERROR',\n                                    messages.nodeMessages['instanceIAMPolicyNOK'],\n                                    instance_role=node.IAMRole,\n                                    node=worker_node_id,\n                                    policy=str(checkIAMPolicy[1]))\n                else:\n                    messages.render('I4',\n                                    'INFO2',\n                                    messages.nodeMessages['instanceIAMPolicyOK'],\n                                    instance_role=node.IAMRole)\n                if checkTrustRelationship:\n                    messages.render('I4',\n                                    'INFO2',\n                                    messages.nodeMessages['instanceProfileThrustOK'],\n                                    instance_role=node.IAMRole)\n                else:\n                    messages.render('I4',\n                                    'ERROR',\n                                    messages.nodeMessages['instanceProfileThrustNOK'],\n                                    instance_role=node.IAMRole)\n            else:\n                messages.render('I3',\n                                'ERROR',\n                                messages.nodeMessages['instanceProfileNOK'])\n\n            messages.render('I2',\n                            'NODASH',\n                            messages.nodeMessages['checkingUserData'])\n            if userdata.check_userdata(node.UserData,\n                                       cluster_name):\n                messages.render('I3',\n                                'INFO',\n                                messages.nodeMessages['userDataOK'])\n            else:\n                messages.render('I3',\n                                'ERROR',\n                                messages.nodeMessages['userDataNOK'])\n\n            node_tags.execute_checks(node, cluster, messages)\n            node_ami.execute_checks(node, cluster, messages)\n            node_cni.execute_checks(node, messages)\n            node_sg_outbound.execute_checks(node, messages)\n            outposts_check.execute_checks(node,messages)\n\n            messages.render('I2',\n                            'NODASH',\n                            messages.networkMessages['checkingNACL'])\n            nacls_basic_rules.execute_checks(node, cluster, messages)\n\n            messages.render('I2',\n                                'NODASH',\n                                messages.nodeMessages['CheckingStsEndpoint'])\n            if sts_endpoint.check_sts_endpoint(node.Region):\n                messages.render('I3',\n                                'INFO',\n                                messages.nodeMessages['CorrectSTSendpoint'], region=node.Region)\n            else:\n                messages.render('I3',\n                                'ERROR',\n                                messages.nodeMessages['MissingSTSendpoint'], region=node.Region)\n            messages.render('I2',\n                            'NODASH',\n                            messages.nodeMessages['checkingImdsEndpoint'])\n            if imds_check.imds_check(node):\n                messages.render('I3',\n                                'INFO',\n                                messages.nodeMessages['ImdsEndpointEnabled'])\n            else:\n                messages.render('I3',\n                                'ERROR',\n                                messages.nodeMessages['ImdsEndpointDisabled'])\n            messages.render('I2',\n                            'NODASH',\n                                messages.nodeMessages['checkingSSMAgent'])\n            if sw_daemons.ssm_agent_check(node.name):\n                messages.render('I3',\n                                'INFO',\n                                messages.nodeMessages['ssmAgentReachable'])\n                if sw_daemons.check_containerd(node.name):\n                    messages.render('I4',\n                                    'INFO2',\n                                    messages.nodeMessages['daemonRunning'], daemon_name='containerd')\n                else:\n                    messages.render('I4',\n                                    'ERROR',\n                                    messages.nodeMessages['daemonNotRunning'], daemon_name='containerd')\n                if float(cluster.version) < 1.23 :\n                    if sw_daemons.check_docker(node.name):\n                        messages.render('I4',\n                                        'INFO2',\n                                        messages.nodeMessages['daemonRunning'], daemon_name='docker')\n                    else:\n                        messages.render('I4',\n                                        'ERROR',\n                                        messages.nodeMessages['daemonNotRunning'], daemon_name='docker')\n                if sw_daemons.check_kubelet(node.name):\n                    messages.render('I4',\n                                    'INFO2',\n                                    messages.nodeMessages['daemonRunning'], daemon_name='kubelet')\n                else:\n                    messages.render('I4',\n                                    'ERROR',\n                                    messages.nodeMessages['daemonNotRunning'], daemon_name='kubelet')\n            else:\n                messages.render('I3',\n                                'WARNING',\n                                messages.nodeMessages['ssmAgentNotReachable'])                  \n                        \n            messages.render('I0',\n                            'NODASH',\n                            messages.generalMessages['thickSeparator'])\n    messages.render('I0',\n                    'NODASH',\n                    messages.generalMessages['executionComplete'])\n    \n    return {\n        'Message': messages.bufferPrettify()\n    }\n\n",
        "Attachment": "attachment.zip"
      },
      "outputs": [
        {
          "Name": "Message",
          "Selector": "$.Payload.Message",
          "Type": "String"
        }
      ]
    }
  ],
  "files": {
    "attachment.zip": {
      "checksums": {
        "sha256": "e6382e49aa045ab043b1c40b43cf8eccd41cf160688fc13cf070c64215f59e50"
      }
    }
  },
  "outputs": [
    "TroubleshootWorkerNode.Message"
  ]
}
