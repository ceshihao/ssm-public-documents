{
  "description": "The **AWSSupport-CollectAmazonConnectContactFlowLog** automation runbook helps to collect the Amazon Connect contact flow logs for a specific contact ID stored in the configured Amazon CloudWatch log group and upload them to a specified Amazon Simple Storage Service (Amazon S3) bucket. To help with the security of the logs gathered from your Amazon Connect contact flow, the automation evaluates the Amazon S3 bucket configuration to determine if the bucket grants public `read` or `write` access permissions, and it is owned by the AWS account specified in the `S3BucketOwnerAccountId` parameter. If your Amazon S3 bucket uses server-side encryption with AWS Key Management Service keys (SSE-KMS), make sure the user or AWS Identity and Access Management (IAM) role being used to run this automation has the `kms:GenerateDataKey` permissions on the AWS KMS key. For more information about the logs generated by your Amazon Connect instance see [Flow logs stored in an Amazon CloudWatch log group](https://docs.aws.amazon.com/connect/latest/adminguide/contact-flow-logs-stored-in-cloudwatch.html).\n\n### Important\nThe CloudWatch Logs Insights queries incur charges based on the amount of data that is queried. Free tier customers are charged only for usage that exceeds service quotas. For more information, see [Amazon CloudWatch Pricing](https://aws.amazon.com/cloudwatch/pricing/).",
  "schemaVersion": "0.3",
  "assumeRole": "{{ AutomationAssumeRole }}",
  "parameters": {
    "AutomationAssumeRole": {
      "type": "AWS::IAM::Role::Arn",
      "description": "(Optional) The Amazon Resource Name (ARN) of the AWS Identity and Access Management (IAM) role that allows Systems Manager Automation to perform the actions on your behalf. If no role is specified, Systems Manager Automation uses the permissions of the user that starts this runbook.",
      "default": ""
    },
    "ContactId": {
      "type": "String",
      "allowedPattern": "^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$",
      "description": "(Required) The ID of the contact that you want to collect Contact Flow Log for."
    },
    "ConnectInstanceAlias": {
      "type": "String",
      "description": "(Required) The Amazon Connect Instance alias.",
      "allowedPattern": "^(?!d-)([\\da-zA-Z]+)([-]*[\\da-zA-Z])*$",
      "maxChars": 45,
      "minChars": 1
    },
    "S3BucketName": {
      "type": "AWS::S3::Bucket::Name",
      "description": "(Required) The Amazon S3 bucket name in your account where you want to upload Contact Flow Log. Make sure that bucket policy does not grant unnecessary read/write permissions to parties that do not need access to the collected logs."
    },
    "S3ObjectPrefix": {
      "type": "String",
      "description": "(Optional) The Amazon S3 object path in the Amazon S3 bucket for an uploaded the Contact Flow Log. For example, if you specify 'CollectedLogs', the log will be uploaded as 's3://your-s3-bucket/CollectedLogs/ContactFlowLog_[ContactId]_[AWSAccountId].gz'. If you do not specify this parameter, the SSM Automation execution ID is used, example: 's3://your-s3-bucket/[automation:EXECUTION_ID]/ContactFlowLog_[ContactId]_[AWSAccountId].gz'. Note: if you specify a value for 'S3ObjectPrefix' and you run this automation using the same [ContactId], the Contact Flow Log will be overwritten.",
      "default": "",
      "allowedPattern": "^$|^[a-zA-Z0-9\\.\\-_\\\\!*'()/]{1,994}$",
      "maxChars": 994
    },
    "S3BucketOwnerAccountId": {
      "type": "String",
      "description": "(Optional) The AWS Account Number that owns the Amazon S3 bucket where you want to upload the Contact Flow Log. If you do not specify this parameter, the runbooks uses the AWS account ID of the user or role in which the Automation runs.",
      "allowedPattern": "^$|^[0-9]{12}$",
      "default": ""
    },
    "StartTimestamp": {
      "type": "String",
      "description": "(Optional) The start date and time for querying the CloudWatch Logs. The format must be 'yyyy-MM-ddTHH:mm:ss' and timezone needs to be UTC. For example, 2023-01-20T00:00:00. It will be 5 days before the current date and time if this parameter is not specified.",
      "default": "",
      "allowedPattern": "^$|^[0-9]{4}-(0[1-9]|1[0-2])-(0[1-9]|[1-2][0-9]|3[0-1])T(2[0-3]|[01][0-9]):[0-5][0-9]:[0-5][0-9]$"
    },
    "EndTimestamp": {
      "type": "String",
      "description": "(Optional) The end date and time for querying the CloudWatch Logs. The format must be 'yyyy-MM-ddTHH:mm:ss' and timezone needs to be UTC. For example, 2023-01-25T00:00:00. It will be the current date and time if this parameter is not specified.",
      "default": "",
      "allowedPattern": "^$|^[0-9]{4}-(0[1-9]|1[0-2])-(0[1-9]|[1-2][0-9]|3[0-1])T(2[0-3]|[01][0-9]):[0-5][0-9]:[0-5][0-9]$"
    }
  },
  "mainSteps": [
    {
      "name": "CheckS3BucketPublicStatus",
      "description": "Checks if the Amazon S3 bucket specified in the \"S3BucketName\" allows anonymous, or public read or write access permissions.",
      "action": "aws:executeScript",
      "timeoutSeconds": 600,
      "onFailure": "Abort",
      "inputs": {
        "Runtime": "python3.8",
        "Handler": "handler",
        "InputPayload": {
          "S3BucketName": "{{ S3BucketName }}",
          "S3BucketOwnerAccountId": "{{ S3BucketOwnerAccountId }}"
        },
        "Script": "# Copyright 2022 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n# SPDX-License-Identifier: LicenseRef-.amazon.com.-AmznSL-1.0\n# Licensed under the Amazon Software License  http://aws.amazon.com/asl/\n\n\nimport boto3\nimport sys\nfrom botocore.errorfactory import ClientError\n\nsys.tracebacklimit = 0\ns3 = boto3.client(\"s3\")\n\n\ndef get_public_access_settings(account_id, bucket_name=None):\n    try:\n        default_value = {\n            # Assumes by default that the configuration allows public access policies\n            \"BlockPublicAcls\": False,\n            \"IgnorePublicAcls\": False,\n            \"BlockPublicPolicy\": False,\n            \"RestrictPublicBuckets\": False,\n        }\n        if bucket_name is None:\n            settings = boto3.client(\"s3control\").get_public_access_block(\n                AccountId=account_id\n            )\n        else:\n            settings = s3.get_public_access_block(\n                Bucket=bucket_name, ExpectedBucketOwner=account_id\n            )\n\n        return settings.get(\"PublicAccessBlockConfiguration\", default_value)\n\n    except ClientError as e:\n        if e.response[\"Error\"][\"Code\"] == \"NoSuchPublicAccessBlockConfiguration\":\n            return default_value\n        else:\n            raise Exception(\n                \"Error: An error occurred when retrieving the Amazon S3 Public Access\"\n                \" Settings - {}\".format(e.response[\"Error\"][\"Message\"])\n            )\n\n    except Exception as e:\n        print(\n            \"Error: An error occurred when retrieving the Amazon S3 Public Access Settings - {}.\"\n            \" The runbook will assume that Public Access Settings are all set to False.\".format(\n                str(e)\n            )\n        )\n        return default_value\n\n\ndef get_block_public_access(bucket_name, account_id):\n    # Assumes by default that the configuration allows public access policies\n    block_ignore_public_acl = False\n    block_ignore_public_policy = False\n\n    try:\n        # Get Block public access settings for Account (account_id)\n        account_settings = get_public_access_settings(account_id)\n\n        if account_settings.get(\"IgnorePublicAcls\"):\n            block_ignore_public_acl = True\n        if account_settings.get(\"RestrictPublicBuckets\"):\n            block_ignore_public_policy = True\n\n        # Get Block public access settings for Bucket (bucket_name)\n        settings = get_public_access_settings(account_id, bucket_name)\n\n        if settings.get(\"IgnorePublicAcls\"):\n            block_ignore_public_acl = block_ignore_public_acl or True\n        if settings.get(\"RestrictPublicBuckets\"):\n            block_ignore_public_policy = block_ignore_public_policy or True\n\n        if block_ignore_public_policy:\n            print(\n                \"Info: Amazon S3 Block Public Access Settings are configured to ignore\"\n                \" any public bucket policy.\"\n            )\n\n        if block_ignore_public_acl:\n            print(\n                \"Info: Amazon S3 Block Public Access Settings are configured to ignore\"\n                \" public ACLs.\"\n            )\n\n        return block_ignore_public_acl, block_ignore_public_policy\n\n    except Exception as e:\n        print(f\"Error: {str(e)}\")\n        return block_ignore_public_acl, block_ignore_public_policy\n\n\ndef is_bucket_policy_public(bucket_name, account_id):\n    # When evaluating a bucket, the code assumes that the bucket is public.\n    try:\n        is_public = s3.get_bucket_policy_status(\n            Bucket=bucket_name, ExpectedBucketOwner=account_id\n        )[\"PolicyStatus\"][\"IsPublic\"]\n\n        if is_public:\n            print(\"Warning: The Amazon S3 bucket policy is marked as public.\")\n            return True\n        else:\n            print(\"Info: The Amazon S3 bucket policy is marked as private.\")\n            return False\n\n    except ClientError as e:\n        if e.response[\"Error\"][\"Code\"] == \"NoSuchBucketPolicy\":\n            print(\"Info: The Amazon S3 bucket has no policy associated.\")\n            return False\n        else:\n            print(\n                \"Error: An error occurred when retrieving the Amazon S3 Bucket Policy Status - {}.\"\n                \" The runbook will assume that the Bucket policy is marked as public.\".format(\n                    str(e)\n                )\n            )\n            return True\n\n    except Exception as e:\n        print(f\"Error: {str(e)}\")\n        return True\n\n\ndef is_bucket_acl_public(bucket_name, account_id):\n    PUBLIC_GROUP = {\"http://acs.amazonaws.com/groups/global/AllUsers\": \"All Users\"}\n    # Anonymous canonical user ID https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html\n    ANONYMOUS_USER = \"65a011a29cdf8ec533ec3d1ccaae921c\"\n    READ_WRITE_PERMISSIONS = (\"READ\", \"FULL_CONTROL\", \"WRITE\")\n    try:\n        bucket_acl_response = s3.get_bucket_acl(\n            Bucket=bucket_name, ExpectedBucketOwner=account_id\n        )\n\n        for grant in bucket_acl_response[\"Grants\"]:\n            uri = grant[\"Grantee\"].get(\"URI\")\n            uid = grant[\"Grantee\"].get(\"ID\")\n\n            if (\n                uri in PUBLIC_GROUP.keys()\n                and grant[\"Permission\"] in READ_WRITE_PERMISSIONS\n            ):\n                print(\"Warning: Bucket ACL is marked as public for All Users.\")\n                return True\n            if uid == ANONYMOUS_USER and grant[\"Permission\"] in READ_WRITE_PERMISSIONS:\n                print(\"Warning: Bucket ACL is marked as public for Anonymous users.\")\n                return True\n\n        print(\"Info: Bucket ACL is marked as private.\")\n        return False\n\n    except ClientError as e:\n        print(\n            \"Error: An error occurred when retrieving the Amazon S3 ACLs - {}.\"\n            \" The runbook will assume the ACL grants public access.\".format(\n                str(e)\n            )\n        )\n        return True\n    except Exception as e:\n        print(f\"Error: {str(e)}\")\n        return True\n\n\ndef handler(event, context):\n    bucket_name = event.get(\"S3BucketName\", \"\")\n    account_id = event.get(\"S3BucketOwnerAccountId\", \"\")\n    \n    if not account_id:\n        account_id = context[\"global:ACCOUNT_ID\"]\n\n    if not bucket_name:\n        return\n\n    try:\n        block_public_policy, block_public_acl = get_block_public_access(\n            bucket_name, account_id\n        )\n\n        if block_public_policy and block_public_acl:\n            # Block and/or account public access settings are configured to ignore both public ACLs and any public bucket policy.'\n            return\n\n        policy_is_public = is_bucket_policy_public(bucket_name, account_id)\n        acl_is_public = is_bucket_acl_public(bucket_name, account_id)\n\n        if (not policy_is_public or block_public_policy) and not acl_is_public:\n            return\n\n        if block_public_acl and not policy_is_public:\n            return\n\n        raise Exception(\n            \"Error: The runbook has found that the S3 bucket could be publicly accessible.\"\n            \" This runbook prevents a file from being uploaded to S3 if it can be potentially\"\n            \" exposed to unauthorized people.\"\n        )\n\n    except Exception as e:\n        raise Exception(f\"Error: An error ocurred when checking if the Amazon S3 bucket allows anonymous, or public read or write access permissions {str(e)}.\")\n"
      },
      "nextStep": "GetUnixTimestamp"
    },
    {
      "name": "GetUnixTimestamp",
      "description": "Checks if the timestamps provided in \"StartTimestamp\" or \"EndTimestamp\" are valid and get the Unix date and time representation for both.",
      "action": "aws:executeScript",
      "timeoutSeconds": 600,
      "onFailure": "Abort",
      "inputs": {
        "Runtime": "python3.8",
        "Handler": "handler",
        "InputPayload": {
          "StartTimestamp": "{{ StartTimestamp }}",
          "EndTimestamp": "{{ EndTimestamp }}"
        },
        "Script": "# Copyright 2022 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n# SPDX-License-Identifier: LicenseRef-.amazon.com.-AmznSL-1.0\n# Licensed under the Amazon Software License  http://aws.amazon.com/asl/\n\nimport sys\nimport datetime\n\nsys.tracebacklimit = 0\n\n\ndef is_timestamp_valid(timestamp):\n    iso_format = \"%Y-%m-%dT%H:%M:%S\"\n    try:\n        datetime.datetime.strptime(timestamp, iso_format)\n        return True\n    except ValueError:\n        return False\n\n\ndef get_unix_time(timestamp):\n    iso_format = \"%Y-%m-%dT%H:%M:%S\"\n    return int(datetime.datetime.strptime(timestamp, iso_format).timestamp())\n\n\ndef handler(event, context):\n    start_timestamp = event[\"StartTimestamp\"]\n    end_timestamp = event[\"EndTimestamp\"]\n\n    # Check both are in ISO format (yyyy-MM-ddTHH:mm:ss)\n    if len(start_timestamp) != 0 and not is_timestamp_valid(start_timestamp):\n        raise Exception(\n            f\"Error: The provided 'StartTimestamp' {start_timestamp} is not in the\"\n            \" format of 'yyyy-MM-ddTHH:mm:ss'.\"\n        )\n\n    if len(end_timestamp) != 0 and not is_timestamp_valid(end_timestamp):\n        raise Exception(\n            f\"Error: The provided 'EndTimestamp' {end_timestamp} is not in the\"\n            \" format of 'yyyy-MM-ddTHH:mm:ss'.\"\n        )\n\n    # Checks if EndTimestamp is larger than StartTimestamp\n    now = datetime.datetime.now()\n    unix_start_timestamp = int((now - datetime.timedelta(days=5)).timestamp())\n    unix_end_timestamp = int(now.timestamp())\n\n    if len(start_timestamp) != 0:\n        unix_start_timestamp = get_unix_time(start_timestamp)\n    if len(end_timestamp) != 0:\n        unix_end_timestamp = get_unix_time(end_timestamp)\n\n    if unix_start_timestamp >= unix_end_timestamp:\n        raise Exception(\"Error: 'StartTimestamp' must be before 'EndTimestamp'.\")\n\n    # Return UNIX timestamps\n    return {\n        \"UnixStartTimestamp\": unix_start_timestamp,\n        \"UnixEndTimestamp\": unix_end_timestamp,\n    }\n"
      },
      "outputs": [
        {
          "Name": "UnixStartTimestamp",
          "Selector": "$.Payload.UnixStartTimestamp",
          "Type": "Integer"
        },
        {
          "Name": "UnixEndTimestamp",
          "Selector": "$.Payload.UnixEndTimestamp",
          "Type": "Integer"
        }
      ],
      "nextStep": "StartQuery"
    },
    {
      "name": "StartQuery",
      "description": "Starts a query log for the provided \"ContactId\" in CloudWatch Logs log group associated with Amazon Connect instance provided in \"ConnectInstanceAlias\". Queries time out after 60 minutes of runtime. If your query times out, reduce the time range being searched. You can view the queries currently in progress as well as your recent query history in the CloudWatch console. For more information see [View running queries or query history](https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CloudWatchLogs-Insights-Query-History.html).",
      "action": "aws:executeScript",
      "timeoutSeconds": 600,
      "onFailure": "Abort",
      "inputs": {
        "Runtime": "python3.8",
        "Handler": "handler",
        "InputPayload": {
          "StartTime": "{{ GetUnixTimestamp.UnixStartTimestamp }}",
          "EndTime": "{{ GetUnixTimestamp.UnixEndTimestamp }}",
          "ConnectInstanceAlias": "{{ ConnectInstanceAlias }}",
          "ContactId": "{{ ContactId }}"
        },
        "Script": "# Copyright 2022 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n# SPDX-License-Identifier: LicenseRef-.amazon.com.-AmznSL-1.0\n# Licensed under the Amazon Software License  http://aws.amazon.com/asl/\n\nimport sys\nimport boto3\n\nsys.tracebacklimit = 0\nlogs = boto3.client(\"logs\")\n\n\ndef start_query(contact_id, instance_alias, starttime, endtime):\n    log_group_name = \"/aws/connect/{}\".format(instance_alias)\n    query_string = 'fields @timestamp, @message | sort @timestamp asc | filter ContactId = \"{}\" | limit 10000'.format(\n        contact_id\n    )\n\n    try:\n        response = logs.start_query(\n            logGroupName=log_group_name,\n            startTime=starttime,\n            endTime=endtime,\n            queryString=query_string,\n            limit=10000,\n        )\n\n        return response[\"queryId\"]\n\n    except Exception as e:\n        raise Exception(f\"Error: {str(e)}\")\n\n\ndef handler(event, context):\n    contact_id = event[\"ContactId\"]\n    instance_alias = event[\"ConnectInstanceAlias\"]\n    starttime = event[\"StartTime\"]\n    endtime = event[\"EndTime\"]\n    query_id = start_query(contact_id, instance_alias, starttime, endtime)\n\n    return {\"QueryId\": query_id}\n"
      },
      "outputs": [
        {
          "Name": "QueryId",
          "Selector": "$.Payload.QueryId",
          "Type": "String"
        }
      ],
      "nextStep": "WaitForQueryCompletion"
    },
    {
      "name": "WaitForQueryCompletion",
      "description": "Waits for the CloudWatch Logs query log for the provided \"ContactId\" to be completed. Queries time out after 60 minutes of runtime. If your query times out, reduce the time range being searched. You can view the queries currently in progress as well as your recent query history in the CloudWatch console. For more information see [View running queries or query history](https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CloudWatchLogs-Insights-Query-History.html).",
      "action": "aws:waitForAwsResourceProperty",
      "onFailure": "Continue",
      "timeoutSeconds": 3900,
      "inputs": {
        "Service": "logs",
        "Api": "GetQueryResults",
        "queryId": "{{ StartQuery.QueryId }}",
        "PropertySelector": "$.status",
        "DesiredValues": [
          "Complete",
          "Failed",
          "Cancelled",
          "Timeout",
          "Unknown"
        ]
      },
      "nextStep": "UploadContactFlowLog"
    },
    {
      "name": "UploadContactFlowLog",
      "description": "Gets query result and upload Contact flow log to the S3 bucket specified in the \"S3BucketName\".",
      "action": "aws:executeScript",
      "timeoutSeconds": 600,
      "onFailure": "Abort",
      "inputs": {
        "Runtime": "python3.8",
        "Handler": "handler",
        "InputPayload": {
          "ContactId": "{{ ContactId }}",
          "QueryId": "{{ StartQuery.QueryId }}",
          "S3BucketName": "{{ S3BucketName }}",
          "S3ObjectPrefix": "{{ S3ObjectPrefix }}",
          "S3BucketOwnerAccountId": "{{ S3BucketOwnerAccountId }}"
        },
        "Script": "# Copyright 2022 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n# SPDX-License-Identifier: LicenseRef-.amazon.com.-AmznSL-1.0\n# Licensed under the Amazon Software License  http://aws.amazon.com/asl/\n\nimport os\nimport sys\nimport boto3\nimport gzip\nimport json\nimport tempfile\n\n\nsys.tracebacklimit = 0\ns3 = boto3.client(\"s3\")\nlogs = boto3.client(\"logs\")\n\n\ndef check_log_events_status(query_id):\n    response = logs.get_query_results(queryId=query_id)\n\n    query_status = response[\"status\"]\n    if query_status == \"Complete\":\n        return\n    else:\n        raise Exception(\n            f\"Error: CloudWatch Logs Insight query failed with status {query_status}.\"\n            \" You can view the queries currently in progress as well as your recent query history\"\n            \" in the CloudWatch console. For more information see [View running queries or query history]\"\n            \"(https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CloudWatchLogs-Insights-Query-History.html).\"\n        )\n\n\ndef get_log_events(query_id):\n    log_events = []\n    try:\n        response = logs.get_query_results(queryId=query_id)\n\n        for event in response[\"results\"]:\n            for event_object in event:\n                if event_object[\"field\"] == \"@message\":\n                    try:\n                        log_events.append(json.loads(event_object[\"value\"]))\n\n                    # Append an event as string if it cannot be parsed\n                    except json.decoder.JSONDecodeError as e:\n                        log_events.append(event_object[\"value\"])\n        return log_events\n\n    except Exception as e:\n        raise Exception(f\"Error: {str(e)}\")\n\n\ndef write_compressed_logs_gzip(log_events, dst_file_path):\n    print(f\"Saving logs to {dst_file_path}\")\n    with gzip.open(dst_file_path, \"wt\") as zipfile:\n        for log_event in log_events:\n            if type(log_event) is dict:\n                json.dump(log_event, zipfile, ensure_ascii=False, indent=2)\n            else:\n                zipfile.write(log_event)\n            zipfile.write(\"\\n\")\n\n\ndef upload_contact_flow_log(filename, bucket, key, account_id):\n    try:\n        args = {\"ExpectedBucketOwner\": account_id, \"ACL\": \"bucket-owner-full-control\"}\n        print(f\"Uploading {filename} to {bucket}/{key} with args {args}\")\n        s3.upload_file(filename, bucket, key, ExtraArgs=args)\n    except Exception as e:\n        raise Exception(\n            f\"Error: An error ocurred when trying to upload the file {str(e)}.\"\n            \" You can view the queries currently in progress as well as your recent query history \"\n            \" in the CloudWatch console. For more information see [View running queries or query history]\"\n            \"(https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CloudWatchLogs-Insights-Query-History.html).\"\n        )\n\n\ndef handler(event, context):\n    contact_id = event[\"ContactId\"]\n    query_id = event[\"QueryId\"]\n    s3_bucket = event[\"S3BucketName\"]\n    object_path = event.get(\"S3ObjectPrefix\", \"\").strip(\"/ \")\n    account_id = event.get(\"S3BucketOwnerAccountId\", \"\")\n\n    if not account_id:\n        account_id = context[\"global:ACCOUNT_ID\"]\n\n    if not object_path:\n        object_path = context[\"automation:EXECUTION_ID\"]\n\n    check_log_events_status(query_id)\n    log_events = get_log_events(query_id)\n    if len(log_events) == 0:\n        raise Exception(\"Error: No entry found for Contact ID: {}\".format(contact_id))\n\n    file_name = \"ContactFlowLog_{}_{}.gz\".format(\n        contact_id, context[\"global:ACCOUNT_ID\"]\n    )\n    temp_dir = tempfile.TemporaryDirectory()\n    tmp_file_path = os.path.join(temp_dir.name, file_name)\n    write_compressed_logs_gzip(log_events, tmp_file_path)\n    s3_object_key = \"{}/{}\".format(object_path, file_name)\n    upload_contact_flow_log(tmp_file_path, s3_bucket, s3_object_key, account_id)\n\n    s3_uri = \"s3://{}/{}\".format(s3_bucket, s3_object_key)\n\n    s3_console_uri = \"https://s3.console.aws.amazon.com/s3/object\"\n    if context[\"global:AWS_PARTITION\"] == \"aws-cn\":\n        s3_console_uri = \"https://console.amazonaws.cn/s3/object\"\n    elif context[\"global:AWS_PARTITION\"] == \"aws-us-gov\":\n        s3_console_uri = \"https://console.amazonaws-us-gov.com/s3/object\"\n\n    s3_console_url = \"{}/{}?region={}&prefix={}\".format(\n        s3_console_uri,\n        s3_bucket,\n        context[\"global:REGION\"],\n        s3_object_key,\n    )\n\n    return {\"S3Uri\": s3_uri, \"S3ConsoleUrl\": s3_console_url}\n"
      },
      "outputs": [
        {
          "Name": "S3Uri",
          "Selector": "$.Payload.S3Uri",
          "Type": "String"
        },
        {
          "Name": "S3ConsoleUrl",
          "Selector": "$.Payload.S3ConsoleUrl",
          "Type": "String"
        }
      ]
    }
  ],
  "outputs": [
    "UploadContactFlowLog.S3Uri",
    "UploadContactFlowLog.S3ConsoleUrl"
  ]
}
